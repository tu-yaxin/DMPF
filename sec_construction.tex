\section{New DMPF constructions}
In this section, we display two new constructions of DMPF in \cref{sec:big_state_DMPF} and \cref{sec:OKVS_based_DMPF} respectively,  that follow the same paradigm introduced in \cref{sec:DMPF_paradigm}. 

\subsection{DMPF paradigm}\label{sec:DMPF_paradigm}
We begin by introducing the DMPF paradigm in \cref{fig:DMPF_paradigm}, which is based on the idea of the DPF construction in \cite{CCS:BoyGilIsh16}. Each key $k_b(b=0,1)$ generated by ${\sf Gen}(1^\lambda,\hat{f}_{A,B})$ can span a depth-$n$ ($n$ is the input length of $\hat{f}_{A,B}$) complete binary tree $T_b$. Each node in either tree $T_b$ is approached by a path starting from the root, which corresponds to a string in $\{0,1\}^{\le n}$ where $0$ stands for going left and $1$ stands for going right. We call a path that correponds to any nonzero input $a\in A$ an accepting path. 

\input{paradigm.tex}

We call the trees $T_0$, $T_1$ the evaluation trees, with each node in the evaluation tree $T_b$ associated with a $\lambda$-bit pseudorandom string $\seed$ string and an $l$-bit pseudorandom $\sign$ string ($l$ is an adjustable parameter). 
The evaluation tree $T_b$ is determined by the $\seed\|\sign$ string at its root, and the set of corrections words $\{CW^{(i)}\}_{1\le i\le n}$ at each layer. $T_b$ can be computed recursively as follows: for $i = 1,2,\dots, n$, given the $(i-1)$th layer of $T_b$ (the 0th layer is the root), for each node $v$ with strings $\seed_v$ and $\sign_v$ in the $(i-1)$th layer, generate its children's $\seed$ and $\sign$ strings denoted by $\seed_0\|\sign_0$ for the left child and $\seed_1\|\sign_1$ for the right child by first setting $\seed_0\|\sign_0\|\seed_1\|\sign_1 = G(\seed_v)$ where $G:\{0,1\}^\lambda\rightarrow \{0,1\}^{2\lambda+2l}$ is a pseudorandom generator, then compute a correction by $C_\seed\|C_{\sign^0}\|C_{\sign^1}\gets{\sf Correct}(x_1\dots x_{i-1}, \sign, CW^{(i)})$ (see line \ref{alg:paradigm_correction}), and in the end correct the seed strings $\seed_0$ and $\seed_1$ by adding $C_\seed$ to both of them, and correct the sign strings $\sign_0$ and $\sign_1$ by adding $C_{\sign_0}$ and $C_{\sign_1}$ respectively. 

We expect the two evaluation trees to satisfy the following important properties: 
\begin{enumerate}
  \item\label{enu:tree_invariance_1} $T_0$ and $T_1$ have identical $\seed$ and $\sign$ strings on every node not lying on any accepting path.
  \item\label{enu:tree_invariance_2} For a node lying on an accepting path, its $\seed$ strings in $T_0$ and $T_1$ are pseudorandom and independent, while its $\sign$ strings are two correlated pseudorandom strings following some  correlation designed by specific realizations. The correlation is an XOR correlation, meaning the two $\sign$ strings should add (by XOR) up to a specific string.  \footnote{In the big-state realization in \Cref{fig:DMPF_big-state} the two $\sign$ strings add up to a unit vector indicating which accepting path the node is on, and in the OKVS-based realization in \Cref{fig:DMPF_OKVS} the two $\sign$ bits add up to 1 if and only if the node is on an accepting path. }
\end{enumerate}  
The first property is equivalent to asking $T_0$ and $T_1$ to have identical strings on every node exiting an accepting path (meaning the node is not on an accepting path but its parent is): if a parent node is associated with the same strings in $T_0$ and $T_1$, then each of its children is associated with the same strings in $T_0$ and $T_1$, and so is each of the nodes in the subtree rooted at the parent node. 
To force the first property, we expect that at each node exiting an accepting path, the correction $C_\seed$ and $C_\sign$ for this node eliminates the difference between its original $\seed\|\sign$ strings generated by the PRG in $T_0$ and $T_1$. 

To force the second property, we expect that at each node on an accepting path, the correction $C_\seed$ for this node should preserve the pseudorandomness and independence of the original $\seed$ strings in $T_0$ and $T_1$, while the correction $C_\sign$ should force the desired correlation of $\sign$ strings in $T_0$ and $T_1$. 

$\Gen(1^\lambda, \hat{f}_{A,B})$ generates the keys $k_0$ and $k_1$, containing $\seed\|\sign$ string at root and correction words for each layer, which determine $T_0$ and $T_1$ respectively. 
At the $i$th layer, $\Gen(1^\lambda, \hat{f}_{A,B})$ first records in the list $V^{(i-1)}$ all the strings in the $i$th layer that need to be corrected, which are the $\seed$ strings of the nodes exiting an accepting path, and the $\sign$ strings of the nodes whose parent is on an accepting path. Then it utilizes ${\sf GenCW}(i,$ $A, V^{(i-1)})$ to generate the hint $CW^{(i)}$ for both parties (line \ref{alg:paradigm_gen_cw}), such that at an node on the $i$th layer, $CW^{(i)}$ along with the node's position and its parent's $\sign$ can be processed by the method $\Correct$ as the desired corrections $C_\seed$ and $C_\sign$ that forces the properties \ref{enu:tree_invariance_1} and \ref{enu:tree_invariance_2}, as discussed before. 
The detailed implementation of the above process will be discussed in the concrete realizations.

After receiving the key $k_b$, party $b$ can evaluate the input $x=x_1\dots x_n$ by calling $\Eval_b(1^\lambda,$ $k_b,x)$. It first parse the key $k_b$ to the $\seed\|\sign$ string at the root and the correction words $\{CW^{(i)}\}_{i\in[n]}$ for each layer, and then iteratively computes the $\seed\|\sign$ strings along the path represented by $x$ in $T_b$. In the $i$th iteration given the $\seed_{i-1}\|\sign_{i-1}$ string for the node represented by $x_1\dots x_{i-1}$, the $\Eval$ method computes the original $\seed$ and $\sign$ strings for the node $x_1\dots x_{i}$ by invoking PRG on $\seed_{i-1}$, and then correct these strings by invoking $\Correct$ on $x_1\dots x_{i-1}$, $\sign_{i-1}$ and $CW^{(i)}$ to obtain the real $\seed_i\|\sign_i$ (see line \ref{alg:paradigm_correction}).  

The paradigm add a convert layer after the last layer of the evaluation tree to convert the strings at the leaf nodes to an element in the output group $\GG$ of $f_{A,B}$. A correction word $CW^{(n+1)}$ is associated with the convert layer. The output at a leaf node $x$ with string $\seed\|\sign$ is generated by first computing a pseudorandom $\GG$-element $G_{\sf conv}(\seed)$, then adding to $G_{\sf conv}(\seed)$ a correction computed by ${\sf ConvCorrect}(x, \sign, CW^{(n+1)})$, and then give a sign $(-1)^b$ depending on the party (see line \ref{alg:paradigm_convert_correction}). If the leaf node is not on any accepting path, then $G_{\sf conv}(\seed)$ and the correction should be the same in $T_0$ and $T_1$, which means the outputs in $T_0$ and $T_1$ at this node should add up to $0_\GG$. On the other hand, if the leaf node is on any accepting path, then the hint $CW^{(n+1)}$ given by $\Gen(1^\lambda, \hat{f}_{A,B})$ should yield corrections that force the outputs in $T_0$ and $T_1$ to add up to the corresponding element in $B$. Such $CW^{(n+1)}$ is correctly generated by $\sf GenConvCW$ (see line \ref{alg:paradigm_gen_convert_cw}). 

To sum up, we provide the key generation $\Gen$, single-input evaluation $\Eval$ and full-domain evaluation $\FullEval$ in the paradigm in \cref{fig:DMPF_paradigm}. The computation involves the following methods which will be realized in the next sections: 
\begin{itemize}
  \item $\sf Initialize$ defines the strings at the roots of $T_0,T_1$.
  \item $\sf GenCW$ computes hints $\{CW^{(1)},\dots CW^{(n)}\}$ associated with $n$ layers that help generate corrections for the strings at the nodes. Two parties use the same set of correction words. 
  \item $\sf GenConvCW$ computes the hint $CW^{(n+1)}$ associated with the convert layer that help genereate corrections for the final output. Two parties use the same set of correction words. 
  \item $\sf Correct$ given a depth-$(i-1)$ parent node, its $\sign$ string and the hint $CW^{(i)}$, outputs an (additive) correction for its children's strings. 
  \item $\sf ConvCorrect$ given a leaf node, its $\sign$ string and the hint $CW^{(n+1)}$, outputs a correction for the final output in the output group $\GG$. 
\end{itemize}

\Yaxin{Mention early termination?}
\subsection{Big-State DMPF}\label{sec:big_state_DMPF}
We display our first instantiation of DMPF in \cref{fig:DMPF_big-state}, basing on the paradigm of DMPF in \cref{fig:DMPF_paradigm}. In the big-state DMPF we set the length $l$ of the $\sign$ string to be $t$, the number of accepting inputs indicated in $\hat{f}_{A,B}$. The evaluation trees $T_0$ and $T_1$ satisfies properties \ref{enu:tree_invariance_1} and \ref{enu:tree_invariance_2}, such that the $\sign$ string at a node stores a share of the unit vector indicating which accepting path this node is on: for a node lying on the $k$th accepting path in the depth-$i$ layer, its $\sign$ strings in $T_0$ and $T_1$ should add up (by bit-wise XOR) to $e_k = 0^{k-1}10^{t-k}$. Then, the (additive) corrections for computing strings at its children generated by line~\ref{alg:big_state_correction} of \cref{fig:DMPF_big-state} equals $CW^{(i)}[k]$, the $k$th entry of the hint $CW^{(i)}$ associated with this layer. According to line~\ref{alg:big_state_gen_cw} in the construction of $\sf GenCW$, if one of the children exits the accepting path, the $\seed$ correction $C_\seed$ will zero out the difference of this child's $\seed$ strings in $T_0$ and $T_1$. Otherwise $C_\seed$ will be a random correction. The $\sign$ corrections $C_{\sign^0}$ and $C_{\sign^1}$ will force the $\sign$ strings at each child to be a share of $0^t$ if this child exits the accepting path, or to be a unit vector indicating the index of the accepting path in the next layer this child lies on. 

For the convert layer, $\sf GenConvCW$ set $CW^{(n+1)}[k]$ to be the correction that makes the $k$th accepting leaf's outputs in $T_0$ and $T_1$ to add up to $B[k]$. 

\input{big-state.tex}

\begin{remark}
  Note that when $t=1$, the big-state DMPF scheme is exactly the DPF scheme in \cite{CCS:BoyGilIsh16}. 
\end{remark}

We informally argue that the correctness of the big-state DMPF holds since properties \ref{enu:tree_invariance_1} and \ref{enu:tree_invariance_2} of $T_0$ and $T_1$ are ensured, which in turn gives correct shares of outputs in the end of evaluation. The security holds since (1) the $\seed||\sign$ string at the root of $T_b$ is independent of $A$ and $B$, and (2) each hint $CW^{(i)}$ is masked by the pseudorandom value determined by the other party's key, which is indistinguishable with a truly random hint. 

The big-state DMPF is computationally secure when the PRGs $G$ and $G_\conv$ are secure. We defer the proof to \Cref{sec:big-state_security_proof}. 

In the end of this section we briefly discuss about the efficiency of the big-state DMPF, which will be discussed in more details in \cref{sec:applications}. Set the na\"ive solution of DMPF that is a sum of $t$ DPFs as a primary benchmark. The ratio of keysize of the big-state DMPF over the na\"ive solution is roughly $(\lambda+2t)/(\lambda+2)>1$, which is close to 1 if $t\ll \lambda$. $\Gen$, $\Eval$ and $\FullEval$ all traverse one evaluation tree while the na\"ive solution traverse $t$ evaluation trees. However, the PRG used in the big-state DMPF have output length $2\lambda+2t$, which means the running time still grows with $t$. In short, the big-state DMPF is faster than the na\"ive solution with the sacrifice of larger keysize. When $t\ll\lambda$, compared to the na\"ive solution, the big-state DMPF has similar keysize and almost $\times t$ speedup in running time. 

\subsection{OKVS-based DMPF}\label{sec:OKVS_based_DMPF}
Next we display our second instantiation of DMPF in \cref{fig:DMPF_OKVS}, basing on the paradigm of DMPF in \cref{fig:DMPF_paradigm}. We call this instantiation the OKVS-based DMPF, since we utilize primitive OKVS (see \cref{sec:prelim_okvs} for introduction). 

In the OKVS-based DMPF, we set the length $l$ of the $\sign$ string to be $1$. The $\sign$ strings at the same node in $T_0$ and $T_1$ will obey the following correlation: they are shares of 1 if this node is on an accepting path and 0 if this node is not on any accepting path. In order to ensure properties \ref{enu:tree_invariance_1} and \ref{enu:tree_invariance_2}, for a parent node on an accepting path, the additive correction $C_\seed, C_{\sign^0}$ and $C_{\sign^1}$ for the strings at its children are determined such that, if one of its children exits the accepting path, then the $\seed$ correction $C_\seed$ should zero out this child's $\seed$ strings in $T_0$ and $T_1$. Otherwise $C_\seed$ will be a random correction. The $\sign$ corrections $C_{\sign^0}$ and $C_{\sign^1}$ will force the $\sign$ strings at each child to be a share of 0 if this child exits the accepting path, or to be a share of 1 if it remains on an accepting path.  

To generate hints $\{CW^{(i)}\}$ to yield the corrections, we utilize the OKVS primitive that can encode key-value pairs to a data structure, which can be later decoded with any stored key to its correponding value. On the depth-$i$ layer, we define the key space to be the set of all depth-$i$ nodes and the value space to be $\{0,1\}^{\lambda+2}$. Each node on this layer that is also on an accepting path needs a $(\lambda+2)$-bit correction, recorded by the value list $V^{(i-1)}$. We encode these $\sf (node,correction)$ pairs (there are up to $t$ such pairs) using an OKVS scheme and set the hint $CW^{(i)}$ to be the encoding (see line~\ref{alg:okvs_gen_cw}). When evaluating, we decode $CW^{(i)}$ using the same OKVS scheme to obtain the correction with regard to any node (see line~\ref{alg:okvs_correction}). 

For the convert layer, $\sf GenConvCW$ set $CW^{(n+1)}$ to be the encoding of $\sf (leaf\,node, output\,correction)$ pairs where each output correction associated with a leaf node makes the leaf's outputs in $T_0$ and $T_1$ add up to the corresponding element in $B$. 

Note that in \cref{fig:DMPF_OKVS} the OKVS scheme $\OKVS_i$ we use for the depth-$i$ layer has key space of size $2^i$ and value space $\{0,1\}^\lambda$. For simplicity we may extend the key space of $\OKVS_i$ to size $2^n$, and realize $\{\OKVS_i\}_{i\in[n]}$ using the same OKVS scheme. For the upmost few layers where $2^i<t$, $\OKVS_i$ may be realized by the most na\"ive way of encoding to a random truth table (see \Cref{sec:prelim_okvs}), which achieves the optimal rate in this occasion. 

\input{OKVS-based.tex}

We informally argue that armed with an OKVS scheme that fails with negligible probabllity, the correctness of the OKVS-based DMPF holds with overwhelming probability since properties \ref{enu:tree_invariance_1} and \ref{enu:tree_invariance_2} are ensured, which in turn gives correct shares of outputs in the end of evaluation. The security holds as long as the OKVS scheme is oblivious. Since the corrections are pseudorandom strings that are masked by pseudorandom values determined by the other party's key, the OKVS scheme won't leak any information about the accepting paths due to its obliviousness. 

The OKVS-based DMPF is computationally secure when the PRGs $G$ and $G_\conv$ are secure and the OKVS schemes $\{\OKVS_i\}_{1\le i\le n}$ and $\OKVS_\conv$ are oblivious. We defer the proof to \Cref{sec:OKVS_DMPF_security_proof}. 

The efficiency of OKVS-based DMPF highly relies on the efficiency of the OKVS scheme it uses. Setting the na\"ive solution as a benchmark, the ratio of keysize of the na\"ive solution over the OKVS-based DMPF is roughly the rate of the OKVS scheme. Similar to the advantage of the big-state DMPF, the OKVS-based DMPF also only traverse one evaluation tree (as opposed to traversing $t$ evaluation trees in the na\"ive solution). However $\Gen$ consumes an OKVS encoding time per layer, and $\Eval$ and $\FullEval$ consume an OKVS decoding/batch decodings per layer. Therefore with an OKVS scheme that has high rate, fast encoding and decoding will result in an OKVS-based DMPF scheme that has small keysize, fast $\Gen$ and $\Eval$/$\FullEval$, respectively. 

\subsection{Comparison}\label{sec:DMPF_comparison}
In this section we summarize the efficiency of the DMPF instantiations we've mentioned and constructed so far. We display the keysize and running time of $\Gen$,$\Eval$ and $\FullEval$ of different DMPF schemes, computed in terms of costs of abstract tools such as PRG, batch code and OKVS. The concrete efficiency will be discussed later in application scenarios in \cref{sec:applications}. 
	\begin{table*}
    \renewcommand\arraystretch{1.5}
    \scalebox{0.945}{
    \begin{threeparttable}
    \caption{Keysize and running time comparison for different DMPF constructions for domain size $N$, $t$ accepting points, output group $\GG$ and computational security parameter $\lambda$. We leave this table with the abstraction of (probabilistic) batch code in the second column and the abstraction of OKVS in the last column, and plug in concrete instantiations later. $m$ in the second column stands for the number of buckets in batch code, and $w$ stands for the number of buckets that each input coordinate is mapped to (we only consider regular degree because this is the case in most instantiations). \Yaxin{Denote $T_G$ as the time for computing $G:\{0,1\}^{\lambda+1}\rightarrow \{0,1\}^{2\lambda+2}$, and $T_{G_{\sf conv}}$ as the time for computing $G_{\sf conv}:\{0,1\}^\lambda\rightarrow \GG$. In the last column, denote $\OKVS$ as the $\OKVS$ scheme used for the first $n$ layers, and $\OKVS_{\sf conv}$ as the $\OKVS$ scheme used for the convert layer.  }}
    \label{tab:formulas_DMPF_comparison}
			\begin{tabular}{ccccc}
				\toprule 
        %Header
				 &Sum of $t$ DPFs & CBC-based DMPF\cite{cryptoeprint:2019/273,cryptoeprint:2019/1084,cryptoeprint:2021/580,cryptoeprint:2017/1142} & Big-state DMPF & OKVS-based DMPF\\

        \midrule

				Keysize & $t(\lambda+2)\log N+t\log\GG$ & $m(\lambda+2)\log(wN/m)+m\log\GG$ & $t(\lambda+2t)\log N+t\log \GG$ &\makecell{ $\log N\times\OKVS$.Codesize\\$+\OKVS_{\sf conv}$.Codesize}\\

        \cline{1-5}
				
				$Gen()$ & $2t\log N\times T_G+2t\times T_{G_{\sf conv}}$ &\makecell{$2m\log(wN/m)\times T_G+2m\times T_{G_{\sf conv}}$\\$\CBC.\Encode+\CBC.\Decode$} &\makecell{$2t\log N\times T_{G^*}$\tnote{1}\\$+t\log N\times (\lambda+t)$-bit-XOR}
        & \makecell{$2t\log N\times T_G+2t\times T_{G_{\sf conv}}$\\$+\log N\times \OKVS.\Encode$\\$+ \OKVS_{\sf conv}.\Encode$} \\

        \cline{1-5}

				$Eval()$ & $t\log N\times T_G+t\times T_{G_{\sf conv}}$ &\makecell{$w\log(wN/m)\times T_G + w\times T_{G_{\sf conv}}$\\Finding all positions the input is mapped to} & \makecell{$\log N\times T_{G^*} + T_{G_{\sf conv}}$\\$+t\log N \times (\lambda+t)$-bit-XOR}&\makecell{$\log N\times T_G$ \\$+\log N\times\OKVS.\Decode$\\ $+\OKVS_{\sf conv}.\Decode$} \\

        \cline{1-5}

				$FullEval()$ & $tN\times T_G+tN\times T_{G_{\sf conv}}$ &\makecell{$wN\times T_G+ wN\times T_{G_{\sf conv}}$\\Finding the full mapping} & \makecell{$N\times T_{G^*} + N\times T_{G_{\sf conv}}$\\$+2tN\times (\lambda+t)$-bit-XOR }& \makecell{$N\times T_G$ \\ $+N\times\OKVS.\Decode$\\$+N\times \OKVS_{\sf conv}.\Decode$} \\
        \bottomrule
			\end{tabular}	
      \begin{tablenotes}
        \item [1] The PRG used in big-state DMPF maps from $\{0,1\}^\lambda$ to $\{0,1\}^{2\lambda+2t}$ whose computation time should grow with $t$. We mark this PRG as $G^*$ and its computation time as $T_{G^*}$. 
        \end{tablenotes}
    \end{threeparttable}
    }
	\end{table*}

Take PCG as a potential application. We care about $\FullEval$ time which is related to PCG seed expanding time. In this aspect, the CBC-based DMPF consumes $w$ times the number of PRGs than big-state DMPF and OKVS-based DMPF, while big-state DMPF's $\FullEval$ time scales with $t^2$ (since the large-bit-XOR time scales with $t^2$) and OKVS-based DMPF in addition consumes large field operations (in OKVS decoding, and maybe more than this). Therefore we expect different DMPF schemes to be the top choice in different choices of $t$ and depending on the computing time of PRG and large field multiplication, and it is likely that the big-state construction performs the best when $t$ is small, while the CBC-based and OKVS-based constructions performs well when $t$ is large. 

\subsection{Distributed Key Generation}
\red{Discussion to be added. }

\subsection{Distributed Multi-Interval Function}
The distributed comparison function (DCF) proposed in \red{\cite{10.1007/978-3-662-46803-6_12,CCS:BoyGilIsh16}} generates shares for any comparison function $f_{<\alpha, \beta}:[N]\rightarrow \GG$ that evaluates to $\beta$ on any input $x<\alpha$ and 0 elsewhere, for a secret threshold $\alpha\in[N]$ and a secret payload $\beta\in \GG$. Previous literatures give efficient and secure construction of DCF that has keysize $\log N(\lambda+2\log |\GG|+2)$ and evaluation time roughly the same as the evaluation time for DPF in the same domain and output group. A DCF for $f_{<\alpha, \beta}$ can be easily converted to a DCF for other comparisons $f_{\le \alpha, \beta}$, $f_{>\alpha,\beta}$ and $f_{\ge \alpha,\beta}$. 

The application of DCF includes distributed public interval containments and spline functions \cite{cryptoeprint:2020/1392,cryptoeprint:2019/1095}, and constructing pseudorandom correlation functions \cite{cryptoeprint:2022/1014}. \Yaxin{Haven't got a complete application list. And does the latter one actually ask for a distributed multi-interval function? }


The previous construction of DCF is done by extending the PRG-based DPF scheme in the following way: it adds an additional string at each node that can be corrected to be used to compute the shared output of the comparison function. Similarly to this conversion, we argue in this section that any DMPF scheme following the DMPF paradigm in \Cref{fig:DMPF_paradigm} can be converted to a distributed multi-interval function, where a multi-interval function is defined as follows: 
\begin{definition}[Multi-Interval Function]
  Given a domain size $N=2^n$ and Abelian group $\GG$, a $k$-interval function $f_{I,B}:[N]\rightarrow \GG$ for $I = \left([a_1,b_1], [a_2,b_2], \dots, [a_k,b_k]\right)$ such that $1\le a_1<b_1<a_2<b_2<\dots<a_k<b_k\le N$ and $B = (\beta_1,\beta_2,\dots,\beta_k)\in \GG^k$ evaluates to $\beta_i$ on any input $x$ in the interval $[a_i,b_i]$ for $1\le i\le k$ and to $0$ on all other inputs. Call the collection of all $k$-interval functions for all $k$ the multi-interval functions. 
\end{definition}
We define a distributed multi-interval function to be in the similar form as DMPF, while its security requires that each key leaks no information about the intervals $I$ and the payloads $B$. 

Note that a distributed multi-interval function can be constructed by summing up $2k$ DCFs, since $f_{I,B} = \sum_{i=1}^k f_{\ge a_i,\beta_i} + \sum_{i=1}^k f_{\ge b_i+1, -\beta_i}$. Therefore we can relate the shares of $f_{I,B}$ (denoted as $[f_{I,B}]$) with the shares of the multi-point function $f_{A',B'}$ where $A' = (a_1,b_1+1,\dots,a_k,b_k+1)$ and $B' = (\beta_1,-\beta_1,\dots,\beta_k,-\beta_k)$: 
\[
  f_{I,B}(x)  = \sum_{a_i\le x}\beta_i - \sum_{b_j<x}\beta_j = \sum_{y\le x}f_{A',B'}(y)
\]
Hence the full-domain evaluation of a multi-interval function can be directly computed from the full-domain evaluation of a DMPF, with the same keysize and approximately the same $\FullEval$ time. 

Next we make additional modifications to convert a DMPF scheme to a distributed multi-interval function, when aiming for the single-input evaluation. For simplicity, in the sequel, we assume the domain is $\{0,1\}^n$ and the output group $\GG$ is the group of $g$-bit binary strings with the group addition being $\oplus$. In this setting $\beta = -\beta$. One can put a little more effort to generalize the conversion to any group. 

For a DMPF scheme for $f_{A',B'}$ following the paradigm in \Cref{fig:DMPF_paradigm}, let each node contains an additional $g$-bit string $\res$, satisfying the following conditions: (1) for each node $v$ on an accepting path of $f_{A',B'}$, let $\res_0$ and $\res_1$ denote its $\res$ strings in the evaluation trees $T_0$ and $T_1$ respectively, then 
\[
  \res_0\oplus \res_1 = \sum_{a_i\text{ is }v\text{ or a descendant of }v}\beta_i + \sum_{b_j\text{ is }v\text{ or a descendant of }v}\beta_j 
\]
(2) for each node $v$ exiting an accepting path, $\res_0\oplus \res_1 = 0^g$. This, combined with the original invariance \ref{enu:tree_invariance_1} for the evaluation tree, indicates that for each node $v$ not on any accepting path, $\res_0\oplus \res_1 = 0^g$. 

Note that the DMPF paradigm in \Cref{fig:DMPF_paradigm} provides the mechanism to control the difference of strings in any node on or exiting an accepting path, by recording the target strings in the list $V^{(i-1)}$ (see lines \ref{alg:paradigm_append_V_case1} and \ref{alg:paradigm_append_V_case2}), generating the correction word $CW$ (line \ref{alg:big_state_gen_cw}), and correcting the target strings by adding corrections generated by $CW$ (line \ref{alg:paradigm_correction_in_gen}). Therefore, one can enforce the above two conditions by additionally recording $\Delta\res^0$ (for the left child) and $\Delta \res^1$ (for the right child) in $V^{(i-1)}$ (corresponds to modifying lines \ref{alg:paradigm_append_V_case1} and \ref{alg:paradigm_append_V_case2}), then generating the proper correction word for $\res$ strings using  modified $\Gen\CW$ and $\Correct$ in order to get the desired (XOR-)correlation. 

The evaluation on an input $x = x[1\dots n]$ sums up all the $\res$ strings on the nodes that takes a left step to exit the path represented by $x$. The formal description is in \Cref{fig:multi-interval_eval}. 
\begin{figure}
  \caption{Modifying the $\Eval$ method in \Cref{fig:DMPF_paradigm} to the $\Eval$ for a distributed multi-interval function. }
  \label{fig:multi-interval_eval}
  \fbox{\parbox{\linewidth}{    
  \begin{algorithmic}[1]
    \Procedure{Eval\(_b\)}{$1^\lambda, k_b,x$}
    \State Parse $k_b = (\seed,\sign,CW^{(1)},CW^{(2)},\cdots,CW^{(n)})$. 
    \State Denote $x=x[1\dots n]$. 
    \State $S = 0^g$. 
    \For{$i = 1$ to $n$}
      \State $C_\seed\|C_{\sign^0}\|C_{\sign^1}\|C_{\res^0}\|C_{\res^1}\gets {\sf Correct}(x[1\dots(i-1)],\sign,CW^{(i)})$. 
      \State Parse $G(\seed) = \seed^0\|\sign^0\|\res^0\|\seed^1\|\sign^1\|\res^1$. 
      \State $\seed\|\sign\gets \seed^{x_i}\oplus C_{\seed}\|\sign^{x_i}\oplus C_{\sign^{x_i}}$. 
      \State $S\gets S \oplus (\res^{1-x_i}\oplus C_{\res^{1-x_i}})$. 
    \EndFor
    \State \Return $S$. 
    \EndProcedure
  \end{algorithmic}
  }}
\end{figure}
We argue that the summation $S$ is a share of $f_{I,B}(x)$. Denote the output of party b as $S_b$. Suppose $J_1 = \{j\in[n]:x[j]=1\}$. Then for any $a\in \{0,1\}^n$, $a\le x$ if and only if there exists $j\in J_1$ such that $a[1\dots(j-1)] = x[1\dots (j-1)]$ and $a[j] = 0$. Therefore $a$ is or is a descendant of the node $v_j = x[1\dots (j-1)]\|0$ in the tree. Hence, 
\[
  \begin{split}
    S_0 + S_1 =& \sum_{j\in J_1} (\res_{v_j, 0}\oplus\res_{v_j,1})\\
    =& \sum_{j\in J_1} \left(\sum_{\substack{a_i\text{ is }v_j\\\text{or is a descendant of }v_j}}\beta_i +\sum_{\substack{b_i\text{ is }v_j\\\text{or is a descendant of }v_j}}\beta_i\right)\\
    =&\sum_{a_i\le x}\beta_i + \sum_{b_i\le x}\beta_i
  \end{split}
\]
where $\res_{v,b}$ denotes the $\res$ string on node $v$ in tree $T_b$. 

The distributed multi-interval function is secure as long as the DMPF for $f_{A',B'}$ is secure, since the ends of the intervals and the payloads are hidden. 

For distributing a $k$-interval function, we can compare our construction with the na\"ive construction that adds up $2k$ DCFs, which has the similar evaluation time of $2k$ DPFs and $\times (1+\frac{\log |\GG|}{\lambda/2+1})$ the keysize of $2k$ DPFs. Note our conversion from DMPF extends each entry of $V^{(i-1)}$ (the input of $\Gen\CW$) from $(\lambda +2l)$-bit to $(\lambda + 2l +2\log |\GG|)$-bit. When the DMPF paradigm is realized by the big-state (where $l=2k$) or the OKVS-based (where $l=1$) DMPF scheme, the keysize of the distributed $k$-interval function is increased to $\times (1+\frac{\log|\GG|}{\lambda/2+l})$ the keysize of DMPF. Therefore the comparison between our construction and the na\"ive construction of distributed $k$-interval function, should be `proportional to' the comparison between our construction and the na\"ive construction of DMPF for $2k$-point functions, which is discussed in \Cref{sec:DMPF_comparison} and \Cref{sec:applications}.

\begin{remark}
  Since the PBC-based DMPF does not follow the DMPF paradigm in \Cref{fig:DMPF_paradigm}, the conversion does not apply. It is not clear to us how to convert the PBC-based DMPF to a distributed multi-interval function. 
\end{remark}