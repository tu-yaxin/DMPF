\section{New DMPF constructions}
In this section, we display two new constructions of DMPF that follow the same paradigm shown in \cref{fig:DMPF_paradigm}. 

We begin by introducing the DMPF paradigm in \cref{fig:DMPF_paradigm}, which is based on the idea of the DPF construction in \cite{CCS:BoyGilIsh16}. Each key $k_b(b=0,1)$ generated by ${\sf Gen}(\hat{f}_{A,B})$ can span a height-$n$ ($n$ is the input length of $\hat{f}_{A,B}$) complete binary tree $T_b$ (call it the evaluation tree) that has $2^n$ leaf nodes, and each of its nodes is associated with a value. Party $b$ can evaluate the input $x=x_1\cdots x_n$ by starting from the root of the evaluation tree, on the $i$th layer going left if $x_i=0$ and going right if $x_i=1$, until reaching a leaf node then computing the result according to the value on this leaf node. 

Each node of this tree is associated with a $\lambda$-bit seed string and a $l$-bit sign string. For a parent node on the $i$th layer with $\seed$ and $\sign$, its children's seeds and signs are generated by $\sf PRG(\seed)\oplus Correction$, where the $\sf Correction$ is determined by the parent node's position, its $\sign$ and a correction word $CW^{(i)}$ associated with that layer. On a leaf node on the last layer, its $\seed$ will generate a random element in the output group, which will be corrected by adding a $\sf Correction$ determined by the leaf node's position, its sign and the last correction word $CW^{(n+1)}$.

The computation of values on evaluation trees $T_0,T_1$ involves the following methods in the paradigm in \cref{fig:DMPF_paradigm}: 
\begin{itemize}
  \item $\sf Initialize$ computes the values on the roots of $T_0,T_1$.
  \item $\sf GenCW$ computes correction words associated with the first $n$ layers, namely $\{CW^{(1)},\cdots CW^{(n)}\}$. Two parties use the same set of correction words. 
  \item $\sf GenConvCW$ computes the correction word $CW^{(n+1)}$ associated with the last layer that'll help genereate the final result in the output group $\GG$. Two parties use the same set of correction words. 
  \item $\sf Correct$ on input a node's position, its $\sign$ and the correction word $CW^{(i)}$ corresponding to its layer, outputs an additive correction for its children's value. 
  \item $\sf ConvCorrect$ on input a leaf node's position, its $\sign$ and the correction word $CW^{(n+1)}$, outputs an additive correction for the final result in the output group $\GG$. 
\end{itemize}

Call any path from the root to a leaf (corresponding to a string in $A$) an accepting path. To force the correctness, we maintain the following invariance on the evaluation trees $T_0$, $T_1$ of the two parties: 
\begin{itemize}
  \item If a node is not on any accepting path, then in $T_0$ and $T_1$ it is associated with the same $\seed$ and $\sign$. 
  \item If a node is on an accepting path, then $T_0$ and $T_1$ assign to it with different signs that controls the corrections on its children (or on the output if it's a leaf node). 
\end{itemize}
If a node is not on any accepting path, then its children are associated with the same values in $T_0$ and $T_1$, and so is the subtree rooted at this node. Hence one only needs to design how to correct children of nodes on the accepting paths. We provides the big-state DMPF and the OKVS-based DMPF basing on this paradigm, that differ mainly on the way of generating and computing corrections. 

%We make the following restrictions on the methods in order to guarantee the invariance on the evaluation trees: 
%${\sf M}(\bar{x},\sign, CW) =\sum_{i=1}^l\sign[i]\cdot{\sf M}(\bar{x},0^{i-1}10^{l-i},CW)$ for all $\sf M\in\{Correct, ConvCorrect\}$, input $\bar{x}$ and $CW$.

\begin{figure*}
  \caption{The paradigm of our DMPF schemes. We leave the sign string length $l$, methods $\sf Initialize, GenCW,$ $\sf GenConvCW, Correct, ConvCorrect$ to be determined by specific constructions. In the paradigm $A^{(i)}$ denotes the list of accepting paths reaching the $i$th layer, and $\seed^{(i)}$ and $\sign^{(i)}$ denote the lists of $\seed$ values and $\sign$ values at the nodes on the $i$th layer that are on accepting paths.}
  \label{fig:DMPF_paradigm}
  \fbox{\parbox{\linewidth}{
  \begin{algorithmic}
    \State \textbf{Public parameters: }
    \State The $t$-point function family $\{f_{A,B}\}$ with $t$ an upperbound of the number of nonzero points, input domain $[N]=\{0,1\}^n$ and the output group $\GG$. 
    \State Suppose there is a public PRG $G:\{0,1\}^\lambda\rightarrow \{0,1\}^{2\lambda+2l}$. Parse $G(x) = G_0(x)\|G_1(x)$ to the left half and right half of the output. 
    \State Suppose there is a public PRG $G_{\sf convert}:\{0,1\}^\lambda\rightarrow \GG$. 
    \item[]
    \Procedure{Gen}{$1^\lambda, \hat{f}_{A,B}$}
    \State Denote $A = (\alpha_1,\cdots,\alpha_t)$ in lexicographically order, $B = (\beta_1,\cdots,\beta_t)$. If $|A|<t$, extend $A$ to size-$t$ with arbitrary $\{0,1\}^n$ strings and $B$ with 0's. 
    \State For $0\le i\le n-1$, let $A^{(i)}$ denote the sorted and deduplicated list of $i$-bit prefixes of strings in $A$. Specifically, $A^{(0)} = [\epsilon]$. 
    \State For $0\le i\le n-1$ and $b=0,1$, initialize empty lists $\seed_b^{(i)}$ and $\sign_b^{(i)}$. 
    \State ${\sf Initialize}(\{\seed_b^{(0)},\sign_b^{(0)}\}_{b=0,1})$. 
    \For{$i=1$ to $n$}
    \State $CW^{(i)}\gets {\sf GenCW}(i,A,\{\seed_b^{(i-1)},\sign_b^{(i-1)}\}_{b=0,1})$. 
      \For{$k = 1$ to $|A^{(i-1)}|$ and $z=0,1$}
        \State Compute $C_{\seed,b}\|C_{\sign^0,b}\|C_{\sign^1,b}\gets {\sf Correct}(A^{(i-1)}[k], \sign_b^{(i-1)}[k], CW^{(i)})$ for $b=0,1$. 
        \If{$A^{(i-1)}[k]\|z\in A^{(i)}$}
        \State Append the first $\lambda$ bit of $G_z(\seed_b^{(i-1)}[k])\oplus(C_{\seed,b}\|C_{\sign^z,b})$ to $\seed_b^{(i)}$ and the rest to $\sign_b^{(i)}$. 
        \EndIf
      \EndFor
    \EndFor
    \State $CW^{(n+1)}\gets{\sf GenConvCW}(A,B,\{\seed_b^{(n)},\sign_b^{(n)}\}_{b=0,1})$. 
    \State Set $k_b \gets (\seed_b^{(0)},\sign_b^{(0)}, CW^{(1)},CW^{(2)},\cdots,CW^{(n+1)})$.
    \State \textbf{return} $(k_0,k_1)$.
    \EndProcedure
    \item[]
    \Procedure{Eval\(_b\)}{$1^\lambda, k_b,x$}
    \State Parse $k_b = ([\seed],[\sign],CW^{(1)},CW^{(2)},\cdots,CW^{(n+1)})$. 
    \State Denote $x=x_1x_2\cdots x_n$. 
    \For{$i = 1$ to $n$}
      \State $C_\seed\|C_{\sign^0}\|C_{\sign^1}\gets {\sf Correct}(x_1\cdots x_{i-1},\sign,CW^{(i)})$.
      \State $\seed||\sign\gets G_{x_i}(\seed)$. 
      \State $\seed\|\sign\gets G_{x_i}(\seed)\oplus(C_{\seed}\|C_{\sign^{x_i}})$. 
    \EndFor
    \State \Return $(-1)^b\cdot \big(G_{\sf convert}(\seed)+{\sf ConvCorrect}(x,\sign,CW^{(n+1)})\big)$. 
    \EndProcedure
    \item[]
    \Procedure{FullEval\(_b\)}{$1^\lambda,k_b$}
    \State Parse $k_b=(\seed^{(0)},\sign^{(0)},CW^{(1)},CW^{(2)},\cdots,CW^{(n+1)})$. 
    \State For $1\le i\le n$, ${\sf Path}^{(i)}\gets$ the lexicographical ordered list of $\{0,1\}^i$. ${\sf Path}^{(0)}\gets[\epsilon]$. 
    \For{$i=1$ to $n$}
      \For{$k = 1$ to $2^{i-1}$}
        \State $C_\seed\|C_{\sign^0}\|C_{\sign^1}\gets {\sf Correct}({\sf Path}{(i-1)}[k],\sign^{(i-1)}[k],CW^{(i)})$.
        \State $\seed^{(i)}[2k]\|\sign^{(i)}[2k]\gets G_0(\seed^{(i-1)}[k])\oplus (C_\seed\|C_{\sign^0})$.
        \State $\seed^{(i)}[2k+1]\|\sign^{(i)}[2k+1]\gets G_1(\seed^{(i-1)}[k])\oplus (C_\seed\|C_{\sign^1})$.
      \EndFor
    \EndFor
    \For{$k = 1$ to $2^n$}
      \State ${\sf Output}[k]\gets (-1)^b\cdot \big(G_{\sf convert}(\seed^{(n)}[k])+{\sf ConvCorrect}({\sf Path}[k],\sign^{(n)}[k],CW^{(n+1)})\big)$.
    \EndFor
    \State\Return $\sf Output$. 
    \EndProcedure
    \end{algorithmic}}}
\end{figure*}

\newpage
\subsection{Big-State DMPF}
Displayed in \cref{fig:DMPF_big-state}.
TBD: explain
\begin{figure}
  \caption{The parameter $l$ and methods' setting that turns the paradigm of DMPF in~\cref{fig:DMPF_paradigm} into the big-state DMPF. }
  \label{fig:DMPF_big-state}
  \fbox{\parbox{\linewidth}{
  \begin{algorithmic}
    \State Set $l\leftarrow t$, the upperbound of $|A|$. 
    \Procedure{Initialize}{$\{\seed_b^{(0)},\sign_b^{(0)}\}_{b=0,1}$}
    \State For $b=0,1$, let $\seed_b^{(0)} = [r_b]$ where $r_b\xleftarrow{\$}\{0,1\}^\lambda$. 
    \State For $b=0,1$, set $\sign_b^{(0)} = [b\|0^{t-1}]$. 
    \EndProcedure
    \item[]
    \Procedure{GenCW}{$i,A,\{\seed_b^{(i-1)},\sign_b^{(i-1)}\}_{b=0,1}$}
    \State Let $\{A^{(i)}\}_{0\le i\le n}$ be defined as in~\cref{fig:DMPF_paradigm}. 
    \State Sample a list $CW$ of $t$ random strings from $\{0,1\}^{\lambda+2t}$.  
    \For{$k = 1$ to $|A^{(i-1)}|$}
      \State Parse $G(\seed_b^{(i-1)}[k]) = \seed_b^0\|\sign_b^0\|\seed_b^1\|\sign_b^1$, for $b=0,1$, $\seed_b^0,\seed_b^1\in\{0,1\}^\lambda$ and $\sign_b^0,\sign_b^1\in\{0,1\}^t$. 
      \State Compute $\Delta\seed^c = \seed_0^c\oplus\seed_1^c$ and $\Delta \sign^c = \sign_0^c\oplus\sign_1^c$ for $c=0,1$. 
      \State Denote ${\sf path}\leftarrow A^{(i-1)}[k]$. 
      \If{both ${\sf path}\|z$ for $z=0,1$ are in $A^{(i)}$}
        \State $d\gets$ the index of ${\sf path}\|0$ in $A^{(i)}$.
        \State $CW[d]\gets r\|\Delta\sign^0\oplus e_d \|\Delta\sign^1\oplus e_{d+1}$ where $r\xleftarrow{\$}\{0,1\}^\lambda$, $e_d = 0^{d-1}10^{t-d}$. 
      \Else
        \State Let $z$ be such that ${\sf path}\|z\in A^{(i)}$. 
        \State $d\gets$ the index of ${\sf path}\|z$ in $A^{(i)}$. 
        \State $CW[d]\gets 
          \begin{cases}
            \Delta \seed^1\|\Delta\sign^0\oplus e_d\|\Delta\sign^1 & z=0\\
            \Delta \seed^0\|\Delta\sign^0\|\Delta\sign^1\oplus e_d & z=1
          \end{cases}$.        
      \EndIf 
    \EndFor
    \State\Return $CW$. 
    \EndProcedure
    \item[]
    \Procedure{GenConvCW}{$A,B,\{\seed_b^{(n)},\sign_b^{(n)}\}$}
      \State Sample a list $CW$ of $t$ random $\GG$-elements.  
      \For{$k = 1$ to $|A|$}
        \State $\Delta g\gets G_{\sf convert}(\seed_0^{(n)}[k]) - G_{\sf convert}(\seed_1^{(n)}[k])$. 
        \State$CW[k]\gets (-1)^{\sign_0^{(n)}[k][k]}(\Delta g-B[k])$.
      \EndFor
      \State \Return $CW$. 
    \EndProcedure
    \item[]
    \Procedure{Correct}{$\bar{x},\sign,CW$}
      \State \Return $C_{\seed}\|C_{\sign^0}\|C_{\sign^1}\gets\sum_{i=1}^t \sign[i]\cdot CW[i]$, where $C_{\sign^0}$ and $C_{\sign^1}$ are $t$-bit. 
    \EndProcedure
    \item[]
    \Procedure{ConvCorrect}{$x,\sign,CW$}
      \State \Return $\sum_{i=1}^t \sign[i]\cdot CW[i]$. 
    \EndProcedure
  \end{algorithmic}}}
\end{figure}


\subsection{OKVS-based DMPF}
Displayed in \cref{fig:DMPF_OKVS}. 
TBD: explain
\newpage
\begin{figure}
  \caption{The parameter $l$ and methods' setting that turns the paradigm of DMPF in~\cref{fig:DMPF_paradigm} into the OKVS-based DMPF. }
  \label{fig:DMPF_OKVS}
  \fbox{\parbox{\linewidth}{
  \begin{algorithmic}
    \State Set $l\leftarrow 1$. 
    \State For $1\le i\le n$, let $\OKVS_i$ be an OKVS scheme (\cref{def:OKVS}) with key space $\mathcal{K} = \{0,1\}^{i-1}$, value space $\mathcal{V} = \{0,1\}^{\lambda+2}$ and input length $t$. 
    \State let $\OKVS_{\sf convert}$ be an OKVS scheme with key space $\mathcal{K} = \{0,1\}^n$, value space $\mathcal{V} = \GG$ and input length $t$. 
    \item[]
    \Procedure{Initialize}{$\{\seed_b^{(0)},\sign_b^{(0)}\}_{b=0,1}$}
    \State For $b=0,1$, let $\seed_b^{(0)} = [r_b\xleftarrow{\$}\{0,1\}^\lambda]$ and $\sign_b^{(0)} = [b]$. 
    \EndProcedure
    \item[]
    \Procedure{GenCW}{$i,A,\{\seed_b^{(i-1)},\sign_b^{(i-1)}\}_{b=0,1}$}
    \State Let $\{A^{(i)}\}_{0\le i\le n}$ be defined as in~\cref{fig:DMPF_paradigm}. 
    \State Sample a list $V$ of $t$ random strings from $\{0,1\}^{\lambda+2}$.  
    \For{$k = 1$ to $|A^{(i-1)}|$}
      \State Parse $G(\seed_b^{(i-1)}[k]) = \seed_b^0\|\sign_b^0\|\seed_b^1\|\sign_b^1$, for $b=0,1$, $\seed_b^0,\seed_b^1\in\{0,1\}^\lambda$ and $\sign_b^0,\sign_b^1\in\{0,1\}$. 
      \State Compute $\Delta\seed^c = \seed_0^c\oplus\seed_1^c$ and $\Delta \sign^c = \sign_0^c\oplus\sign_1^c$ for $c=0,1$. 
      \State Denote ${\sf path}\leftarrow A^{(i-1)}[k]$. 
      \If{both ${\sf path}\|z$ for $z=0,1$ are in $A^{(i)}$}
        \State $V[k]\gets r\|\Delta\sign^0\oplus 1\|\Delta\sign^1\oplus 1$, where $r\xleftarrow{\$}\{0,1\}^\lambda$. 
      \Else
        \State Let $z$ be such that ${\sf path}\|z\in A^{(i)}$. 
        \State $V[k]\gets \Delta \seed^1\|\Delta\sign^0\oplus (1-z)\|\Delta\sign^1\oplus z$.        
      \EndIf
    \EndFor
    \State \Return $\OKVS_i.\Encode(\{A^{(i-1)}[k], V[k]\}_{1\le k\le |A^{(i-1)}|})$. 
    \EndProcedure
    \item[]
    \Procedure{GenConvCW}{$A,B,\{\seed_b^{(n)},\sign_b^{(n)}\}$}
      \State Sample a list $V$ of $t$ random $\GG$-elements. 
      \For{$k = 1$ to $|A|$}
        \State $\Delta g\gets G_{\sf convert}(\seed_0^{(n)}[k]) - G_{\sf convert}(\seed_1^{(n)}[k])$. 
        \State$V[k]\gets (-1)^{\sign_0^{(n)}[k][k]}(\Delta g-B[k])$.
      \EndFor 
      \State \Return $\OKVS_{\sf convert}(\{A[k], V[k]\}_{1\le k\le t})$. 
    \EndProcedure
    \item[]
    \Procedure{Correct}{$\bar{x}, \sign,CW$}
      \State\Return $C_{\seed}\|C_{\sign^0}\|C_{\sign^1}\gets\sign\cdot\OKVS_i.\Decode(CW, \bar{x})$, where $C_{\sign^0}$ and $C_{\sign^1}$ are bits. 
    \EndProcedure
    \item[]
    \Procedure{ConvCorrect}{$x,\sign,CW$}
      \State \Return $\sign\cdot\OKVS_{\sf convert}.\Decode(CW,x)$. 
    \EndProcedure
  \end{algorithmic}}}
\end{figure}
\Yaxin{One point: the $\row$ matrix of the current layer contains the $\row$ matrix of the previous layers, which might be useful for speedup. }

\subsection{Comparison}
%Comparison table dependent to PRG \& $\FF$-MUL(list the formulas?)\\
%analyze tradeoff\\
%distributed gen advantage
\Cref{tab:formulas_DMPF_comparison} displays the keysize, running time of $\Gen$,$\Eval$ and $\FullEval$ for different DMPF schemes, computed in terms of costs of abstract tools such as PRG, batch code and OKVS. \Yaxin{We can plug in the actual costs of these tools after carrying out a complete experiment. }
	\begin{table*}
    \renewcommand\arraystretch{1.5}
    \caption{Keysize and running time comparison for different DMPF constructions for domain size $N$, $t$ accepting points and computational security parameter $\lambda$. The convert layer is ignored for now. We leave this table with the abstraction of (probabilistic) batch code in the second column and the abstraction of OKVS in the last column, and plug in concrete instantiations later. $m$ in the second column stands for the number of buckets used in batch code, and $w$ stands for the number of buckets that each input coordinate is mapped to (we only consider regular degree because this is the case in most instantiations). }
    \label{tab:formulas_DMPF_comparison}
			\begin{tabular}{ccccc}
				\toprule 
        %Header
				 &Sum of $t$ DPFs & Batch code DMPF\cite{cryptoeprint:2019/273,cryptoeprint:2019/1084,cryptoeprint:2021/580,cryptoeprint:2017/1142} & Big-state DMPF & OKVS-based DMPF\\

        \midrule

				Keysize & $t(\lambda+2)\log N$ & $m\lambda\log(wN/m)$ & $t(\lambda+2t)\log N$ &\makecell{ $\log N\times\OKVS$ code size}\\

                \cline{1-5}
				
				$Gen()$ & $2t\log N\times $ PRG &\makecell{$2m\log(wN/m)\times $PRG\\Finding a matching of $t$ inputs to $m$ buckets} &$2t\log N\times$PRG$^*$\footnote{The PRG used in big-state DMPF maps from $\{0,1\}^\lambda$ to $\{0,1\}^{2\lambda+2t}$ whose computation time should grow with $t$. } & \makecell{$2t\log N\times$PRG, \\$\log N\times \OKVS.\Encode$} \\

                \cline{1-5}

				$Eval()$ & $t\log N\times $PRG &\makecell{$w\log(wN/m)\times $PRG\\Finding all buckets an input is mapped to} & $\log N\times$PRG$^*$ &\makecell{$\log N\times$PRG, \\$\log N\times\OKVS.\Decode$} \\

                \cline{1-5}

				$FullEval()$ & $tN\times$PRG &\makecell{$wN\times$PRG\\Finding the input sequence in every bucket} & $N\times$PRG$^*$ & \makecell{$N\times$PRG, \\ $N\times\OKVS.\Decode$} \\

        \bottomrule
			\end{tabular}	
	\end{table*}
    \Yaxin{Note that the PRG in big-state DMPF maps from $\{0,1\}^\lambda$ to $\{0,1\}^{2\lambda+2t}$. }

  \Yaxin{Take PCG as a potential application. We care about $\FullEval$ time which is related to PCG seed expanding time. In this aspect, the batch code DMPF consumes $d\times$PRGs than big-state DMPF and OKVS-based DMPF, while big-state DMPF's $\FullEval$ time scales with $t$ and OKVS-based DMPF in addition consumes large field multiplications (in OKVS decoding, and maybe more than this). Therefore we expect different DMPF schemes to be the top choice in different choices of $t$ and depending on the computing time of PRG and large field multiplication. }

  
\subsection{Distributed Key Generation}