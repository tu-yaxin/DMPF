\section{New DMPF constructions}
In this section, we present two new constructions of DMPF in \cref{sec:big_state_DMPF} and \cref{sec:OKVS_based_DMPF} respectively,  that follow the same template introduced in \cref{sec:DMPF_template}. 

\subsection{DMPF template}\label{sec:DMPF_template}
We begin by introducing the DMPF template in \cref{fig:DMPF_template}, which is which is inspired by the tree-structured construction of DPF in \cite{BoyGilIsh16}. In the template, the length parameter $l$ and methods $\sf Initialize$, $\sf GenCW$, $\sf GenConvCW$, $\sf Correct$, $\sf ConvCorrect$ are left to be determined by specific constructions in the later sections. 

\input{DMPF_template.tex}
\input{DMPF_template_contd.tex}

\paragraph{High-level overview. }Each key $k_b(b=0,1)$ generated by ${\sf Gen}(1^\lambda,\hat{f}_{A,B})$ spans a depth-$n$ ($n$ is the input length of $\hat{f}_{A,B}$) complete binary tree $T_b$, referred to as the `evaluation tree'. Each node in $T_b$ is approached by a path starting from the root, which corresponds to a string in $\{0,1\}^{\le n}$ where $0$ stands for going left and $1$ stands for going right. We call a path that corresponds to any nonzero input $a\in A$ an accepting path. 

Every node in the evaluation tree $T_b$ is associated with a $\lambda$-bit pseudorandom $\seed$ string and an $l$-bit pseudorandom $\sign$ string ($l$ is an adjustable parameter). 
All strings in evaluation tree $T_b$ is determined by the $\seed\|\sign$ string at its root, and the set of corrections words $\{\CW^{(i)}\}_{1\le i\le n}$ at each layer,  computed layer by layer by the following steps: 
\begin{enumerate}[itemsep=5pt]
    \item For each node $v$ with strings $\seed_v$ and $\sign_v$ in the $(i-1)$th layer, generate its children's $\seed$ and $\sign$ strings ($\seed_0\|\sign_0$ for the left child and $\seed_1\|\sign_1$ for the right child) by first setting $\seed_0\|\sign_0\|\seed_1\|\sign_1 = G(\seed_v)$ where $G:\{0,1\}^\lambda\rightarrow \{0,1\}^{2\lambda+2l}$ is a pseudorandom generator.
    \item Compute a correction by $C_\seed\|C_{\sign^0}\|C_{\sign^1}\gets{\sf Correct}(x_1\dots x_{i-1}, \sign, \CW^{(i)})$.
    \item Add $C_\seed$ to $\seed_0$ and $\seed_1$, and add $C_{\sign_0},C_{\sign_1}$ to $\sign_0,\sign_1$ respectively in order to correct them to satisfy desired correlations. 
\end{enumerate}
We expect the strings in $T_0$ and $T_1$ satisfy the following correlations, which are the core of this template: 
\begin{enumerate}
  \item\label{enu:tree_invariance_1}$T_0$ and $T_1$ have identical $\seed$ and $\sign$ strings on every node not lying on any accepting path.
  \item\label{enu:tree_invariance_2} For a node lying on an accepting path, its $\seed$ strings in $T_0$ and $T_1$ are pseudorandom and independent, while its $\sign$ strings are two correlated pseudorandom strings following some  correlation designed by specific realizations. The correlation is an XOR correlation, meaning the two $\sign$ strings should add (by XOR) up to a specific string. %\addtocounter{footnote}{-1}\stepcounter{footnote}
  \footnote[1]{In the big-state realization in \Cref{fig:DMPF_big-state} the two $\sign$ strings add up to a unit vector indicating which accepting path the node is on, and in the OKVS-based realization in \Cref{fig:DMPF_OKVS} the two $\sign$ bits add up to 1 if and only if the node is on an accepting path. }
\end{enumerate}  
The first property is equivalent to asking $T_0$ and $T_1$ to have identical strings on every node exiting an accepting path (i.e., the node that is not on an accepting path but its parent is): if a parent node is associated with the same strings in $T_0$ and $T_1$, then each of its children is associated with the same strings in $T_0$ and $T_1$, and so is each of the nodes in the subtree rooted at the parent node. 
To force the first property, we expect that at each node exiting an accepting path, the correction $C_\seed$ and $C_\sign$ for this node eliminates the difference between its original $\seed\|\sign$ strings generated by the PRG in $T_0$ and $T_1$. 
To force the second property, we expect that at each node on an accepting path, the correction $C_\seed$ for this node should preserve the pseudorandomness and independence of the original $\seed$ strings in $T_0$ and $T_1$, and meanwhile the correction $C_\sign$ should force the desired correlation of $\sign$ strings in $T_0$ and $T_1$. 

$\Gen(1^\lambda, \hat{f}_{A,B})$ generates the keys $k_0$ and $k_1$, containing $\seed\|\sign$ string at root and correction words for each layer, which determine $T_0$ and $T_1$ respectively. 
At the $i$th layer, $\Gen(1^\lambda, \hat{f}_{A,B})$ first records in the list $V^{(i-1)}$ all the strings in the $i$th layer that need to be corrected, which are the $\seed$ strings of the nodes exiting an accepting path, and the $\sign$ strings of the nodes whose parent is on an accepting path. Then it runs the method ${\sf GenCW}(i,$ $A, V^{(i-1)})$ to generate $\CW^{(i)}$ for both parties (line \ref{alg:template_gen_cw}), such that at a node $v$ on the $i$th layer, the method $\Correct(v,\sign$ of $v$'s parent$,\CW^{(i)})$ outputs the desired corrections $C_\seed$ and $C_\sign$ for $v$. 

After receiving the key $k_b$, party $b$ can evaluate the input $x=x_1\dots x_n$ by calling $\Eval_b(1^\lambda,$ $k_b,x)$. It first parse the key $k_b$ to the $\seed\|\sign$ string at the root and the correction words $\{\CW^{(i)}\}_{i\in[n]}$ for each layer, then computes the $\seed\|\sign$ strings along the path represented by $x$ in $T_b$ layer by layer. On the $i$th layer, given the $\seed_{i-1}\|\sign_{i-1}$ string for the node  $x_1\dots x_{i-1}$, the $\Eval$ method computes $G(\seed_{i-1})$ and the correction $\Correct(x_1\dots x_{i-1},\sign_{i-1},\CW^{(i)})$ to obtain $\seed_i\|\sign_i$ (see line \ref{alg:template_correction_in_gen}).  
\paragraph{Converting strings to $\GG$ elements. }After the $n$th layer of the evaluation tree, the template adds a convert layer associated with $\CW^{(n+1)}$ to convert the strings at a leaf node to an element in the output group $\GG$ of $f_{A,B}$. At a leaf node $x$ with string $\seed\|\sign$, the final output is computed by invoking $G_{\sf conv}(\seed)$ to get a pseudorandom $\GG$-element and invoking  ${\sf ConvCorrect}(x, \sign, \CW^{(n+1)})$ to get a correction on $G_{\sf conv}(\seed)$ (see line \ref{alg:template_convert_correction}). $\CW^{(n+1)}$ generated in $\Gen$ (see line \ref{alg:template_gen_convert_cw}) will force the following correlations of outputs in $T_0$ and $T_1$ , similar to the correlations in the first $n$ layers: 
If the leaf node is not on any accepting path, then the outputs in $T_0$ and $T_1$ at this node should add up to $0_\GG$. On the other hand, if the leaf node is on any accepting path, then the corrected outputs in $T_0$ and $T_1$ should add up to the corresponding element in $B$.

To sum up, we provide the key generation $\Gen$, single-input evaluation $\Eval$ and full-domain evaluation $\FullEval$ in the template in \cref{fig:DMPF_template}. The computation involves the following methods which will be realized in the next sections: 
\begin{itemize}
  \item $\sf Initialize$ defines the strings at the roots of $T_0,T_1$.
  \item $\sf GenCW$ computes hints $\{\CW^{(1)},\dots \CW^{(n)}\}$ associated with $n$ layers that help generate corrections for the strings at the nodes. Two parties use the same set of correction words. 
  \item $\sf GenConvCW$ computes the hint $\CW^{(n+1)}$ associated with the convert layer that help genereate corrections for the final output. Two parties use the same set of correction words. 
  \item $\sf Correct$ given a depth-$(i-1)$ parent node, its $\sign$ string and the hint $\CW^{(i)}$, outputs an (additive) correction for its children's strings. 
  \item $\sf ConvCorrect$ given a leaf node, its $\sign$ string and the hint $\CW^{(n+1)}$, outputs a correction for the final output in the output group $\GG$. 
\end{itemize}
\begin{remark}[Early termination optimization]
When the output group $\GG$ of $f_{A,B}$ has small size, we can apply the early termination optimization to reduce the $\log(\lambda/\log|\GG|)$ layers of the evaluation tree, as proposed in \cite{BoyGilIsh16} for optimizing DPF in the same situation. %We can pack adjacent output values to form an equivalent $t$-point function $f_{A^*, B^*}$ with a  larger output group $\GG^*$ of size roughly $\lambda$ and a reduced domain size to about $\frac{N\log|\GG|}{\lambda}$. 
\end{remark}

\subsection{Big-State DMPF}\label{sec:big_state_DMPF}
In this section We display our first instantiation of DMPF in \cref{fig:DMPF_big-state} referred to as the big-state DMPF, based on the template of DMPF in \cref{fig:DMPF_template}. 

\paragraph{High-level overview.}In the big-state DMPF we set the length $l$ of the $\sign$ string to be $t$, the number of accepting inputs indicated in $\hat{f}_{A,B}$. The evaluation trees $T_0$ and $T_1$ satisfy correlations \ref{enu:tree_invariance_1}) and \ref{enu:tree_invariance_2}), in a way that the $\sign$ string at a node stores a share of the unit vector indicating which accepting path this node is on: for a node lying on the $k$th accepting path in the depth-$i$ layer, its $\sign$ strings in $T_0$ and $T_1$ should add up (by bit-wise XOR) to $e_k = 0^{k-1}10^{t-k}$. The desired correlations are ensured by the design of $\Gen\CW$ and $\Correct$ in \cref{fig:DMPF_big-state}. On the $i$th layer, $\Gen\CW$ generates a list $\CW^{(i)}$ of $(\lambda+2t)$-bit strings, where $\CW^{(i)}[k]$ is the intended correction for children of the node on the $k$th accepting path on the $(i-1)$th layer. 
$\Correct$ derives the intended correction using the $\sign$ string through a straightforward inner-product.

For the convert layer, $\sf GenConvCW$ set $\CW^{(n+1)}[k]$ to be the correction that makes the $k$th accepting leaf's outputs in $T_0$ and $T_1$ to add up to $B[k]$, which can be extracted by $\Conv\Correct$ using the $\sign$ string and again a straightforward inner-product.  

\input{DMPF_big_state}

\begin{remark}
  Note that when $t=1$, the big-state DMPF scheme is exactly the DPF scheme in \cite{BoyGilIsh16}. 
\end{remark}

We informally argue that the correctness of the big-state DMPF holds since correlations \ref{enu:tree_invariance_1}) and \ref{enu:tree_invariance_2}) of $T_0$ and $T_1$ are ensured, which in turn gives the correct shares of outputs in the end of evaluation. The security holds since (1) the $\seed||\sign$ string at the root of $T_b$ is independent of $A$ and $B$, and (2) each hint $\CW^{(i)}$ is masked by the pseudorandom value determined by the other party's key, which is indistinguishable with a truly random hint. We defer the formal proof to our full version. 

\begin{theorem}[Big-state DMPF is correct and secure]
    Let $\epsilon_G$ and $\epsilon_{G_\conv}$ denote the distinguishing advantage of the PRG $G$ and $G_\conv$ respectively. Then the big-state DMPF scheme in \cref{fig:DMPF_big-state} is perfectly correct and $\epsilon$-secure against any n.u.p.p.t. adversary, where $\epsilon = 2tn\epsilon_G+2t\epsilon_{G_\conv}$. 
\end{theorem}

\paragraph{Efficiency of big-state DMPF.}In the end of this section we briefly discuss about the efficiency of the big-state DMPF, which will be discussed in more detail later. The ratio of keysize of the big-state DMPF over the na\"ive DMPF is roughly $(\lambda+2t)/(\lambda+2)>1$, but is close to 1 if $t\ll \lambda$. For the running time of $\Gen$, $\Eval$ and $\FullEval$ where the total time of PRG invocations is a significant factor, big-state DMPF traverses only one evaluation tree while the na\"ive DMPF traverses $t$ evaluation trees, which means the number of PRG invocations in big-state DMPF is only $\frac1t\times$ the number of PRG invocations in na\"ive DMPF. However, the PRG used in the big-state DMPF have output length $2\lambda+2t$. Therefore, when $t\ll\lambda$, compared to the na\"ive solution, we expect big-state DMPF to have a significant speedup in running time, while preserving a similar keysize compared to na\"ive DMPF.  

\subsection{OKVS-based DMPF}\label{sec:OKVS_based_DMPF}
Next we display our second instantiation of DMPF in \cref{fig:DMPF_OKVS} referred to as the OKVS-based DMPF, based on the template of DMPF in \cref{fig:DMPF_template}. We call this instantiation the OKVS-based DMPF, since we utilize the primitive OKVS (see \cref{sec:prelim_okvs} for an introduction). 

\paragraph{High-level overview. }In the OKVS-based DMPF, we set the length $l$ of the $\sign$ string to be $1$. The evaluation trees $T_0$ and $T_1$ satisfy correlations \ref{enu:tree_invariance_1}) and \ref{enu:tree_invariance_2}), in a way that 
the $\sign$ string at a node stores a share of 1 if this node is on an accepting path, a share of and 0 if this node is not on any accepting path. 
In order to ensure desired correlations, at a parent node on an accepting path, the corrections $C_\seed, C_{\sign^0}$ and $C_{\sign^1}$ for the strings at its children are determined by the following if-else sentences: if one of its children exits the accepting path, then the $\seed$ correction $C_\seed$ should zero out this child's $\seed$ strings in $T_0$ and $T_1$. Otherwise $C_\seed$ will be a random correction. The $\sign$ corrections $C_{\sign^0}$ and $C_{\sign^1}$ will force the $\sign$ strings at each child to be a share of 0 if this child exits the accepting path, or to be a share of 1 if it remains on an accepting path.  

In \cref{fig:DMPF_OKVS}, $\Gen\CW$ generates $\{\CW^{(i)}\}$ which can derive the intended corrections by running $\Correct$. They utilize the OKVS primitive that can encode key-value pairs to a data structure, which can be later decoded with any stored key to its corresponding value. On the $(i-1)$th layer, we define the key space to be the set of all nodes on this layer and the value space to be $\{0,1\}^{\lambda+2}$. Each node on this layer that is also on an accepting path needs a $(\lambda+2)$-bit correction for its children, recorded by the value list $V^{(i-1)}$. $\Gen\CW$ sets $\CW^{(i)}$ to be the encoding of these $\sf (node,correction)$ pairs (there are up to $t$ such pairs) using an OKVS scheme. Given $\CW^{(i)}$ and any parent node on the $(i-1)$th layer, $\Correct$ obtains the intended corrections by decoding $\CW^{(i)}$ using the same OKVS scheme. 

For the convert layer, $\sf GenConvCW$ set $\CW^{(n+1)}$ to be the encoding of $\sf (leaf\,node, output\,correction)$ pairs where each output correction associated with a leaf node makes the leaf's outputs in $T_0$ and $T_1$ add up to the corresponding element in $B$. 

Note that in \cref{fig:DMPF_OKVS} the OKVS scheme $\OKVS_i$ we use for the $i$th layer has key space of size $2^i$ and value space $\{0,1\}^{\lambda+2}$. For uniformity we may extend the key space of $\OKVS_i$ to size $2^n$, and realize $\{\OKVS_i\}_{i\in[n]}$ using the same OKVS scheme. For the upmost few layers where $2^i<t$, $\OKVS_i$ may be realized by the most na\"ive way of encoding to a random truth table, which achieves the optimal rate. 

\input{OKVS-based.tex} 
%\Yaxin{The $\row$ matrix of the current layer contains the $\row$ matrix of the previous layers, which might be useful for speedup. }

We informally argue that armed with an OKVS scheme that fails with negligible probabllity, the correctness of the OKVS-based DMPF holds with overwhelming probability since correlations \ref{enu:tree_invariance_1}) and \ref{enu:tree_invariance_2}) of $T_0$ and $T_1$ are ensured, which in turn gives correct shares of outputs in the end of evaluation. The security holds as long as the OKVS scheme is oblivious. Since the corrections are pseudorandom strings that are masked by pseudorandom values determined by the other party's key, the OKVS scheme won't leak any information about the accepting paths due to its obliviousness. 
We defer the formal proof to \Cref{sec:OKVS_DMPF_security_proof}. 

\begin{theorem}[OKVS-based DMPF is correct and secure]
Let $\epsilon_G$ and $\epsilon_{G_\conv}$ denote the distinguishing advantage of the PRG $G$ and $G_\conv$ respectively. Suppose in the OKVS-based DMPF scheme in \Cref{fig:DMPF_OKVS}, for $1\le i\le n$ the $\OKVS_i$ scheme is perfectly correct\footnote[2]{Note that an imperfectly correct OKVS scheme can always be converted to a perfectly correct one by resampling the public randomness that causes error. },   $\epsilon_{\OKVS_i}$-oblivious, and the $\OKVS_{\conv}$ scheme is perfectly correct, $\epsilon_{\OKVS_\conv}$-oblivious. Then the big-state DMPF scheme in \Cref{fig:DMPF_big-state} is perfectly correct and $\epsilon$-secure against any n.u.p.p.t. adversary, where $\epsilon = 2tn\epsilon_G+2t\epsilon_{G_\conv} +\sum_{i=1}^n\epsilon_{\OKVS_i}+\epsilon_{\OKVS_\conv}$. 
\end{theorem}

\paragraph{Efficiency of OKVS-based DMPF.}The efficiency of OKVS-based DMPF highly relies on the efficiency of the OKVS scheme it uses. The ratio of keysize of the na\"ive DMPF over the OKVS-based DMPF is roughly the rate of the OKVS scheme. As for the PRG invocations in running time of $\Gen$, $\Eval$ and $\FullEval$, similar to the big-state DMPF, the OKVS-based DMPF only traverses one evaluation tree, leading to an $\frac{1}{t}\times $ overall PRG invocation time compared to the na\"ive DMPF. However the PRG invocations not necessarily dominates running time in OKVS-based DMPF. In addition to PRG invocations, $\Gen$ uses an OKVS encoding per layer, and $\Eval$ ($\FullEval$) uses an OKVS (batch) decoding per layer. Therefore with an OKVS scheme that has high rate, fast encoding and (batch) decoding will result in an OKVS-based DMPF scheme that has faster $\Gen$ and $\Eval$ ($\FullEval$) time while preserving a similar keysize, compared to na\"ive DMPF. 

\subsection{Comparison}\label{sec:DMPF_comparison}
\begin{table*}
    \renewcommand\arraystretch{1.5}
    %\scalebox{1}{
    \begin{threeparttable}
    \caption{Keysize and running time comparison for different DMPF constructions for domain size $N$, $t$ accepting points, output group $\GG$, statistical security parameter $\lambda_\stat$, and computational security parameter $\lambda$. $T_G$ is the time for computing $G:\{0,1\}^{\lambda+1}\rightarrow \{0,1\}^{2\lambda+2}$, and $T_{G_{\sf conv}}$ is the time for computing $G_{\sf conv}:\{0,1\}^\lambda\rightarrow \GG$. We leave this table with the abstraction of PBC in the second column and the abstraction of OKVS in the last column. In the second column, $\PBC$ is the PBC scheme, where $m$ stands for the number of codewords, and $w$ stands for the number of codewords that each input coordinate is encoded to. In the last column, $\OKVS$ is the OKVS scheme used for the first $n$ layers, and $\OKVS_{\sf conv}$ is the OKVS scheme used for the convert layer.  }
    \label{tab:formulas_DMPF_comparison}
    \begin{tabular}{ccccc}
        \toprule 
        %Header
	&Na\"ive DMPF & PBC-based DMPF & Big-state DMPF & OKVS-based DMPF\\

        \midrule

	Keysize & $t(\lambda+2)\log N+t\log\GG$ & $m(\lambda+2)\log(wN/m)+m\log\GG$ & $t(\lambda+2t)\log N+t\log \GG$ &\makecell{ $\log N\times\OKVS$.Codesize\\$+\OKVS_{\sf conv}$.Codesize}\\

        \cline{1-5}
				
	$\Gen$ & $2t\log N\times T_G+2t\times T_{G_{\sf conv}}$ &\makecell{$2m\log(wN/m)\times T_G+2m\times T_{G_{\sf conv}}$\\$+\PBC.\Encode+\PBC.\Schedule$} &\makecell{$2t\log N\times T_{G^*}$\tnote{1}\\$+t\log N\times (\lambda+t)$-bit-XOR}
        & \makecell{$2t\log N\times T_G+2t\times T_{G_{\sf conv}}$\\$+\log N\times \OKVS.\Encode$\\$+ \OKVS_{\sf conv}.\Encode$} \\

        \cline{1-5}

	$\Eval$ & $t\log N\times T_G+t\times T_{G_{\sf conv}}$ &\makecell{$w\log(wN/m)\times T_G + w\times T_{G_{\sf conv}}$\\$+\PBC.\Position$} & \makecell{$\log N\times T_{G^*} + T_{G_{\sf conv}}$\\$+t\log N \times (\lambda+t)$-bit-XOR}&\makecell{$\log N\times T_G$ \\$+\log N\times\OKVS.\Decode$\\ $+\OKVS_{\sf conv}.\Decode$} \\

        \cline{1-5}

	$\FullEval$ & $tN\times T_G+tN\times T_{G_{\sf conv}}$ &\makecell{$wN\times T_G+ wN\times T_{G_{\sf conv}}$\\$+N\times\PBC.\Position$} & \makecell{$N\times T_{G^*} + N\times T_{G_{\sf conv}}$\\$+2tN\times (\lambda+t)$-bit-XOR }& \makecell{$N\times T_G+N\times T_{G_\conv}$ \\  $+N\times\OKVS.\Decode$\\$+N\times \OKVS_{\sf conv}.\Decode$} \\
        \bottomrule
			\end{tabular}	
      \begin{tablenotes}
        \item [1] The PRG used in big-state DMPF maps from $\{0,1\}^\lambda$ to $\{0,1\}^{2\lambda+2t}$ whose computation time grows with $t$. We mark this PRG as $G^*$ and its computation time as $T_{G^*}$. 
        \end{tablenotes}
    \end{threeparttable}
    %}
	\end{table*}
% Relevant table links: \url{https://docs.google.com/spreadsheets/d/1ZQ7gVK5qio_hPJb9X6aub5PldPaRPrL0/edit##gid=980732839}{\textcolor{blue}{previous analytic results}}, \url{https://docs.google.com/spreadsheets/d/1vZYNrjgahzA_T3KLg8AXg5Z1uveOiIV4VT7tGB9BkCk/edit##gid=0}{\textcolor{blue}{experimental results}}. 

In this section we summarize the efficiency of the DMPF instantiations we've mentioned and constructed so far. We display the keysize and running time of $\Gen$,$\Eval$ and $\FullEval$ of different DMPF schemes, computed in terms of costs of abstract tools such as PRG, PBC and OKVS. 


We expect the PBC-based, big-state and OKVS-based DMPF to have slightly larger keysize than the na\"ive construction, but much faster evaluation time (for both $\Eval$ and $\FullEval$). The evaluation time of the na\"ive construction is dominated by the overall PRG invocation time, which scales linearly with $t$, while the evaluation time for the rest have number of PRG invocations independent to $t$, but with different extra overheads. 

For the PBC-based DMPF, the overall PRG invocation time is roughly $\frac{w}{t}\times $ the overall PRG invocation time of na\"ive DMPF, but it has an invocation of $\sf PBC.Position$ in $\Eval$ and $N$ invocations of $\sf PBC.Position$ in $\FullEval$. 

For the big-state DMPF, the PRG's output length is dependent to $t$ and so is the total PRG invocation time. Implementing PRG with AES, the overall PRG invocation time in big-state DMPF is roughly $\frac{\lambda+t}{t\lambda+t}\times $ the overal PRG invocation time of na\"ive DMPF. In addition, big-state DMPF has extra cost of long string XOR's scaling with $t^2$. Therefore we expect big-state DMPF to perform well when $t$ is small, and become much less competitive when $t$ is large.

For the OKVS-based DMPF, the overall PRG invocation time is roughly $\frac{1}{t}\times $ the overall PRG invocation time of na\"ive DMPF, but it has an invocation of $\sf OKVS.Decode$ in $\Eval$ and $N$ invocations of $\sf OKVS.Decode$ in $\FullEval$. 

Generally speaking, we expect different DMPF schemes to be the top choice for different regimes of $N$ and $t$. Their concrete performance will be discussed later in \cref{sec:implementation}. 

\subsection{Distributed Key Generation}
The key generation of a DMPF scheme requires the existence of a trusted third party. To drop this assumption, we consider distributed key generation that benefits many applications. The distributed key generation of DMPF can be done by a generic secure 2-party computation, but we are interested in the constructions that saves more online communication cost. We measure the communication cost by counting the number of single-bit $\AND$ gates in the circuit computed by secure 2-party computation. 

\cite{cryptoeprint:2017/827} gives such a distributed key generation protocol for DPF with feasible domain size that pushes all PRG evaluations to offline and local computation, while using $O(1)$-round secure 2-party computation for a $O(\lambda)$-size circuit, for each of the $n$ layers in the evaluation tree. In the end of the protocol the parties simultaneously computes $\FullEval$ of the DPF. 

For a DMPF scheme with feasible domain size following the template in~\Cref{fig:DMPF_template} which features a similar tree-like structure, we can push the PRG evaluations to local computation in the distributed key generation, by extending the idea of~\cite{cryptoeprint:2017/827}. 

In $\Gen$ of~\Cref{fig:DMPF_template}, for each $i\in[n]$ and $i>\lceil\log t\rceil$ (otherwise directly applying secure 2-party computation to the key generation process would be efficient as well), $\CW^{(i)}$ can be viewed as computed by the following steps: 
\begin{enumerate}
    \item Compute the list $\Delta^{(i-1)}:=G(\seed_0^{(i-1)})\oplus G(\seed_1^{(i-1)})$ where $\seed_b^{(i-1)}$ denotes the length-$2^{i-1}$ list of $\seed$ strings on the $(i-1)$th layer of $T_b$. $\Delta^{(i-1)}$ is a length-$2^{i-1}$ list of $(2\lambda+2l)$-bit strings.
    \item Extract the nonzero entries $\Delta^{(i-1)}|_{A^{(i-1)}}:=\{\Delta^{(i-1)}[x]:x\in A^{(i-1)}\}$ on the accepting paths (note that by the construction of $T_0$ and $T_1$, the entries corresponding to nodes not on any accepting path are zero strings).
    \item Convert $\Delta^{(i-1)}|_{A^{(i-1)}}$ to the value list $V^{(i-1)}$.
    \item Compute $\CW^{(i)}\gets \Gen\CW(i,A,V^{(i-1)})$.
\end{enumerate}
The PRG evaluations only appear in the first step of computing $\Delta^{(i-1)}$, and in the distributed key generation protocol we push step (1) to local computation such that party $b$ computes list $\Delta_b^{(i-1)}:=G(\seed_b^{(i-1)})$ which is a share of $\Delta^{(i-1)}$. 
Then the protocol computes step (2), (3), and (4) with online communication, after which the parties switch to another offline phase to locally generate the corrected strings $\seed_b^{(i)}$ and $\sign_b^{(i)}$ on the $i$th layer of $T_b$, basing on the $\CW^{(i)}$ they just computed. 

We'll first outline such a distributed key generation protocol for the DMPF template (\Cref{fig:DMPF_template}) that requires secure arithmetic operations (specifically solving linear equation systems), and then focus on a concrete and more practical protocol for big-state DMPF (\Cref{fig:DMPF_big-state}) that uses secure computation over Boolean circuits. The two approaches achieves steps (2) differently but run generic secure 2-party computation for steps (3) and (4). 

\paragraph{A distributed key generation protocol for the DMPF template}View every $(2\lambda+2l)$-bit string as a distinct element in a field $\FF$ such that the addition group $(\FF,+)$ is isomorphic to $(\{0,1\}^{2\lambda+2l},\oplus)$. Then $\Delta^{(i-1)}$ is a vector in $\FF^{2^{i-1}}$ that has at most $t$ nonzero entries whose indices are in $A^{(i-1)}$. The job of step (2) translates to extracting all nonzero entries in a sparse vector whose indices are known to be $A^{(i-1)}$. To achieve this, we make use of a parity matrix $H\in\FF^{t\times 2^{i-1}}$ of any MDS linear error correcting code of length $2^{i-1}$ and distance $t+1$ over field $\FF$ \cite{MR2253870}. Party $b$ locally computes $v_b:=H\cdot \Delta_b^{(i-1)}$, and the two parties use secure 2-party computation to solve the linear equation system $H|_{A^{(i-1)}}\cdot x = v_0+v_1$ whose coefficient matrix $H|_{A^{(i-1)}}$ is the submatrix of $H$ obtained by restricting to the columns indexed by $A^{(i-1)}$. Since all nonzero entries of $\Delta^{(i-1)}$ are also indexed by $A^{(i-1)}$, the solution of this system is $\Delta^{(i-1)}|_{A^{(i-1)}}$ as desired. 

The above protocol requires solving a linear equation system for $t$ variables using secure 2-party computation at each of $n$ layers. For the sake of better concrete efficiency, we present protocol for big-state DMPF that gets rid of the secure arithmetic operations. 

\paragraph{A distributed key generation protocol for the big-state DMPF scheme}For a more concise illustration, let's suppose the $t$-point function is $f_A$ such that $f_A(x) = 1$ if and only if $x\in A$, and neglect the convert layer. The distributed key generation protocol for big-state DMPF along with its ideal functionality are presented in~\Cref{fig:DMPF_distrgen}, with two sub-protocols $\Pi_{\sf SharedIPD}$ and $\Pi_\CW$. $\Pi_{\sf big\hyp state\hyp DKG}$ computes the list $\Delta^{(i-1)}|_{A^{(i-1)}}$ by utilizing the meaningful $\sign$ strings in big-state DMPF, and with the help of the sub-protocol $\Pi_{\sf SharedIPD}$ which computes additive shares of the inner-product of a unit vector and a vector of hamming weight $\le t$. The sub-protocol $\Pi_\CW$ then computes the value list $V^{(i-1)}$ and the desired $\CW^{(i)}$. 
We formally describe the ideal functionality and instantiation of $\Pi_{\sf SharedIPD}$ in~\Cref{fig:shared_ipd}, and the ideal functionality of $\Pi_\CW$ in~\Cref{fig:CW} whose instantiation is by generic secure 2-party computation which is neglected. 
\input{DMPF_distributed_gen_shared_ipd}

\input{DMPF_distributed_gen_CW}

\input{DMPF_distributed_gen}

We show that all of our instantiations are secure in the semi-honest setting and in the OT hybrid model. The security proofs are delayed to~\Cref{sec:security-proof-distrgen}. 
\begin{lemma}[$\Pi_{\sf SharedIPD}$ is secure]\label{lem:shared—ipd-security}
    The construction $\Pi_{\sf SharedIPD}$ realizes the ideal functionality $\mathcal{F}_{\sf SharedIPD}$ in~\Cref{fig:shared_ipd} against semi-honest adversaries in the OT hybrid model. 
\end{lemma}
\begin{theorem}[$\Pi_{\sf big\hyp state\hyp DKG}$ is secure]\label{thm:big-state-DKG-security}
Set the security parameters $s$ and $s'$ in $\Pi_{\sf SharedIPD}$ such that $\max\{(1-1/\mathrm{e})^s,s2^{-s'}\}\cdot nt(2\lambda+2t)\le 2^{-\lambda_\stat}$. The construction $\Pi_{\sf big\hyp state\hyp DKG}$ realizes the ideal functionality $\mathcal{F}_{\sf big\hyp state\hyp DKG}$ in~\Cref{fig:DMPF_distrgen} with perfect security and $1-2^{-\lambda_\stat}$ correctness against semi-honest adversaries in the OT hybrid model. 
\end{theorem}

In the rest of this section, we analyze the concrete efficiency of the protocol $\Pi_{\sf big\hyp state\hyp DKG}$, in terms of local operations and online communication costs measured by the number of $\sf AND$ gates computed by secure 2-party computation. Since big-state DMPF performing well for the range $t\ll 2^n$ as shown in~\Cref{sec:implementation}, let's consider the same setting.

The local operations are dominated by the PRG evaluations for each of the $n$ layers in line~9 of~\Cref{fig:DMPF_distrgen}. In total, each party locally runs $2^n$ PRG evaluations, each mapping a $\lambda$-bit string to a $(2\lambda+2t)$-bit string. 

The online communication costs for each of the $n$ layers consist of $t(2\lambda+2t)$ invocations of $\Pi_{\sf SharedIPD}$ in parallel, followed by an invocation of $\Pi_\CW$. According to the construction in~\Cref{fig:shared_ipd}, each invocation of $\Pi_{\sf SharedIPD}$ computes $ss't$ $\AND$ gates by secure 2-party computation, arranged in a depth-$\log(ss't)$ circuit. An invocation of $\Pi_\CW$ 
corresponding to $t$ iterations of line~13 to line~17 in~\Cref{fig:DMPF_template} followed by the $\Gen\CW$ method in~\Cref{fig:DMPF_big-state} can be computed by a circuit of $t(4\lambda+6t+n+6)$ $\sf AND$ gates with depth $\log n+3$. In total, for each of the $n$ layers, a circuit of size about $ss't^2(2\lambda+2t)$ and depth $\log(ss'tn)$ is computed by secure 2-party computation. 

\iffalse
\begin{remark}
    One may also hope for an efficient distributed key generation for the PBC-based DMPF, since its $\Gen$ just contains PBC encoding followed by different DPFs. However the PBC encoding, e.g., instantiated by cuckoo hashing, requires random access among $\Omega(t)$ elements, where $t$ is the number of the DMPF accepting inputs, and the RAM model is not for free. \Yaxin{`RAM model is not for free' is just my impression. Please directly edit it if people could make it more rigorous. }
\end{remark}
\fi

\subsection{Distributed Multi-Interval Function}
In this section we showed that our DMPF template in \cref{fig:DMPF_template} can be easily converted to a template for distributed multi-interval function (DMIF), following the same idea of converting DPF to distributed comparison function (DCF) in \cite{10.1007/978-3-662-46803-6_12,BoyGilIsh16}. Both DMIF and its special-case DCF have various applications including distributed public interval containments, distributed spline functions \cite{cryptoeprint:2020/1392,cryptoeprint:2019/1095}, and constructing pseudorandom correlation functions \cite{cryptoeprint:2022/1014}. 

DCF generates shares for any comparison function $f_{<\alpha, \beta}:[N]\rightarrow \GG$ that evaluates to $\beta$ on any input $x<\alpha$ and 0 elsewhere, for a secret threshold $\alpha\in[N]$ and a secret payload $\beta\in \GG$. Previous literatures give efficient and secure construction of DCF that has keysize $\log N(\lambda+2\log |\GG|+2)$ and evaluation time roughly the same as the evaluation time for DPF in the same domain and output group. A DCF for $f_{<\alpha, \beta}$ can be easily converted to a DCF for other comparisons $f_{\le \alpha, \beta}$, $f_{>\alpha,\beta}$ and $f_{\ge \alpha,\beta}$. 

The previous construction of DCF is done by extending the binary-tree-based DPF scheme in the following way: it adds an additional string at each node that can be corrected to be used to compute the shared output of the comparison function. Similarly to this conversion, we argue in this section that any DMPF scheme following the DMPF template in \Cref{fig:DMPF_template} can be converted to a distributed multi-interval function, where a multi-interval function is defined as follows: 
\begin{definition}[Multi-Interval Function]
  Given a domain size $N=2^n$ and Abelian group $\GG$, a $k$-interval function $f_{I,B}:[N]\rightarrow \GG$ for $I = \left([a_1,b_1], [a_2,b_2], \dots, [a_k,b_k]\right)$ such that $1\le a_1<b_1<a_2<b_2<\dots<a_k<b_k\le N$ and $B = (\beta_1,\beta_2,\dots,\beta_k)\in \GG^k$ evaluates to $\beta_i$ on any input $x$ in the interval $[a_i,b_i]$ for $1\le i\le k$ and to $0$ on all other inputs. Call the collection of all $k$-interval functions for all $k$ the multi-interval functions. 
\end{definition}
We define a distributed multi-interval function to be in the similar form as DMPF, while its security requires that each key leaks no information about the intervals $I$ and the payloads $B$. 

Note that a distributed multi-interval function can be constructed by summing up $2k$ DCFs, since $f_{I,B} = \sum_{i=1}^k f_{\ge a_i,\beta_i} + \sum_{i=1}^k f_{\ge b_i+1, -\beta_i}$. Therefore we can relate the shares of $f_{I,B}$ (denoted as $[f_{I,B}]$) with the shares of the multi-point function $f_{A',B'}$ where $A' = (a_1,b_1+1,\dots,a_k,b_k+1)$ and $B' = (\beta_1,-\beta_1,\dots,\beta_k,-\beta_k)$: 
\[
  f_{I,B}(x)  = \sum_{a_i\le x}\beta_i - \sum_{b_j<x}\beta_j = \sum_{y\le x}f_{A',B'}(y)
\]
Hence the full-domain evaluation of a multi-interval function can be directly computed from the full-domain evaluation of a DMPF, with the same keysize and approximately the same $\FullEval$ time. 

Next we make additional modifications to convert a DMPF scheme to a distributed multi-interval function, when aiming for the single-input evaluation. For simplicity, in the sequel, we assume the domain is $\{0,1\}^n$ and the output group $\GG$ is the group of $g$-bit binary strings with the group addition being $\oplus$. In this setting $\beta = -\beta$. One can put a little more effort to generalize the conversion to any group. 

For a DMPF scheme for $f_{A',B'}$ following the template in \Cref{fig:DMPF_template}, let each node contains an additional $g$-bit string $\res$, satisfying the following conditions: (1) for each node $v$ on an accepting path of $f_{A',B'}$, let $\res_0$ and $\res_1$ denote its $\res$ strings in the evaluation trees $T_0$ and $T_1$ respectively, then 
\[
  \res_0\oplus \res_1 = \sum_{a_i\text{ is }v\text{ or a descendant of }v}\beta_i + \sum_{b_j\text{ is }v\text{ or a descendant of }v}\beta_j 
\]
(2) for each node $v$ exiting an accepting path, $\res_0\oplus \res_1 = 0^g$. This, combined with the original invariance \ref{enu:tree_invariance_1} for the evaluation tree, indicates that for each node $v$ not on any accepting path, $\res_0\oplus \res_1 = 0^g$. 

Note that the DMPF template in \Cref{fig:DMPF_template} provides the mechanism to control the difference of strings in any node on or exiting an accepting path, by recording the target strings in the list $V^{(i-1)}$ (see lines \ref{alg:template_append_V_case1} and \ref{alg:template_append_V_case2}), generating the correction word $\CW$ (line \ref{alg:big_state_gen_cw}), and correcting the target strings by adding corrections generated by $\CW$ (line \ref{alg:template_correction_in_gen}). Therefore, one can enforce the above two conditions by additionally recording $\Delta\res^0$ (for the left child) and $\Delta \res^1$ (for the right child) in $V^{(i-1)}$ (corresponds to modifying lines \ref{alg:template_append_V_case1} and \ref{alg:template_append_V_case2}), then generating the proper correction word for $\res$ strings using  modified $\Gen\CW$ and $\Correct$ in order to get the desired (XOR-)correlation. 

The evaluation on an input $x = x[1\dots n]$ sums up all the $\res$ strings on the nodes that takes a left step to exit the path represented by $x$. The formal description is in \Cref{fig:multi-interval_eval}. 
\begin{figure}
  \caption{Modifying the $\Eval$ method in \Cref{fig:DMPF_template} to the $\Eval$ for a distributed multi-interval function. }
  \label{fig:multi-interval_eval}
  \fbox{\parbox{\linewidth}{    
  \begin{algorithmic}[1]
    \Procedure{Eval\(_b\)}{$1^\lambda, k_b,x$}
    \State Parse $k_b = (\seed,\sign,\CW^{(1)},\CW^{(2)},\cdots,\CW^{(n)})$. 
    \State Denote $x=x[1\dots n]$. 
    \State $S = 0^g$. 
    \For{$i = 1$ to $n$}
      \State $C_\seed\|C_{\sign^0}\|C_{\sign^1}\|C_{\res^0}\|C_{\res^1}\gets {\sf Correct}(x[1\dots(i-1)],\sign,\CW^{(i)})$. 
      \State Parse $G(\seed) = \seed^0\|\sign^0\|\res^0\|\seed^1\|\sign^1\|\res^1$. 
      \State $\seed\|\sign\gets \seed^{x_i}\oplus C_{\seed}\|\sign^{x_i}\oplus C_{\sign^{x_i}}$. 
      \State $S\gets S \oplus (\res^{1-x_i}\oplus C_{\res^{1-x_i}})$. 
    \EndFor
    \State \Return $S$. 
    \EndProcedure
  \end{algorithmic}
  }}
\end{figure}
We argue that the summation $S$ is a share of $f_{I,B}(x)$. Denote the output of party b as $S_b$. Suppose $J_1 = \{j\in[n]:x[j]=1\}$. Then for any $a\in \{0,1\}^n$, $a\le x$ if and only if there exists $j\in J_1$ such that $a[1\dots(j-1)] = x[1\dots (j-1)]$ and $a[j] = 0$. Therefore $a$ is or is a descendant of the node $v_j = x[1\dots (j-1)]\|0$ in the tree. Hence, 
\[
  \begin{split}
    S_0 + S_1 =& \sum_{j\in J_1} (\res_{v_j, 0}\oplus\res_{v_j,1})\\
    =& \sum_{j\in J_1} \left(\sum_{\substack{a_i\text{ is }v_j\\\text{or is a descendant of }v_j}}\beta_i +\sum_{\substack{b_i\text{ is }v_j\\\text{or is a descendant of }v_j}}\beta_i\right)\\
    =&\sum_{a_i\le x}\beta_i + \sum_{b_i\le x}\beta_i
  \end{split}
\]
where $\res_{v,b}$ denotes the $\res$ string on node $v$ in tree $T_b$. 

The distributed multi-interval function is secure as long as the DMPF for $f_{A',B'}$ is secure, since the ends of the intervals and the payloads are hidden. 

For distributing a $k$-interval function, we can compare our construction with the na\"ive construction that adds up $2k$ DCFs, which has the similar evaluation time of $2k$ DPFs and $\times (1+\frac{\log |\GG|}{\lambda/2+1})$ the keysize of $2k$ DPFs. Note our conversion from DMPF extends each entry of $V^{(i-1)}$ (the input of $\Gen\CW$) from $(\lambda +2l)$-bit to $(\lambda + 2l +2\log |\GG|)$-bit. When the DMPF template is realized by the big-state (where $l=2k$) or the OKVS-based (where $l=1$) DMPF scheme, the keysize of the distributed $k$-interval function is increased to $\times (1+\frac{\log|\GG|}{\lambda/2+l})$ the keysize of DMPF. Therefore the comparison between our construction and the na\"ive construction of distributed $k$-interval function, should be `proportional to' the comparison between our construction and the na\"ive construction of DMPF for $2k$-point functions, which is discussed in \Cref{sec:DMPF_comparison} and \Cref{sec:implementation}.

%\begin{remark}
  %Since the PBC-based DMPF does not follow the DMPF template in \Cref{fig:DMPF_template}, the conversion does not apply. It is not clear to us how to convert the PBC-based DMPF to a distributed multi-interval function. 
%\end{remark}