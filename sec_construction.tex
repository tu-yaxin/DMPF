\section{New DMPF constructions}
In this section, we display two new constructions of DMPF in \cref{sec:big_state_DMPF} and \cref{sec:OKVS_based_DMPF} respectively,  that follow the same paradigm introduced in \cref{sec:DMPF_paradigm}. 

\subsection{DMPF paradigm}\label{sec:DMPF_paradigm}
We begin by introducing the DMPF paradigm in \cref{fig:DMPF_paradigm}, which is based on the idea of the DPF construction in \cite{CCS:BoyGilIsh16}. Each key $k_b(b=0,1)$ generated by ${\sf Gen}(1^\lambda,\hat{f}_{A,B})$ can span a depth-$n$ ($n$ is the input length of $\hat{f}_{A,B}$) complete binary tree $T_b$. Each node in either tree $T_b$ is approached by a path starting from the root, which corresponds to a string in $\{0,1\}^{\le n}$ where $0$ stands for going left and $1$ stands for going right. We call a path that correponds to any nonzero input $a\in A$ an accepting path. 

We call the trees $T_0$, $T_1$ the evaluation trees. Each node in the evaluation tree $T_b$ is associated with a $(\lambda+l)$-bit pseudorandom string $\seed\|\sign$ (the $\lambda$-bit $\seed$ and $l$-bit $\sign$ are defined in line \ref{alg:paradigm_seed_sign}). The two evaluation trees satisfies the following important properties: 
\begin{enumerate}
  \item\label{enu:tree_invariance_1} $T_0$ and $T_1$ have identical strings on every node except for the nodes lying on accepting paths.
  \item\label{enu:tree_invariance_2} For a node lying on an accepting path, its $\seed$ strings in $T_0$ and $T_1$ are pseudorandom and independent, while its $\sign$ strings are pseudorandom and follow some correlation (the correlation is designed by specific instantiations). 
\end{enumerate}  

Party $b$ can evaluate the input $x=x_1\cdots x_n$ by calling $\Eval_b(1^\lambda,$ $k_b,x)$, which first parse the key $k_b$ to the $\seed\|\sign$ string at the root together with $n$ hints $\{CW^{(i)}\}_{i\in[n]}$, for the depth-$i$ layer $(1\le i\le n)$ respectively. $\Eval_b(1^\lambda, k_b,x)$ traverses $T_b$ along the path indicated by $x$, starting from the root, and at a depth-$(i-1)$ node with string $\seed\|\sign$ generates its children's strings by first computing the $(2\lambda+2l)$-bit pseudorandom string $G(\seed)$ where $G:\{0,1\}^\lambda\rightarrow \{0,1\}^{2\lambda+2l}$ is a pseudorandom generator, then adding to $G(\seed)$ a correction computed by ${\sf Correct}(x_1\dots x_{i-1}, \sign, CW^{(i)})$ (see line \ref{alg:paradigm_correction}), and then assign the left $(\lambda+l)$-bit string to its left child and the rest to its right child. In particular, the additive correction for the $\seed$ strings of two children nodes are the same ($C_\seed$ in line \ref{alg:paradigm_correction}) , but those for the $\sign$ strings of two children nodes are different $(C_{\sign^0}$ for the left child and $C_{\sign^1}$ for the right child) in order to force the desired correlation of $\sign$ strings. 

It is $\Gen(1^\lambda, \hat{f}_{A,B})$'s job to generate appropriate strings for roots of $T_0$ and $T_1$ and hints $\{CW^{(i)}\}$ for all layers that maintains the properties \ref{enu:tree_invariance_1} and \ref{enu:tree_invariance_2}. At the depth-$i$ layer, $\Gen(1^\lambda, \hat{f}_{A,B})$ utilizes ${\sf GenCW}(i,$ $A, \{\seed_b^{(i-1)}, \sign_b^{(i-1)}\}_{b=0,1})$ to generate the hint $CW^{(i)}$ for both parties (line \ref{alg:paradigm_gen_cw}), where $\seed_b^{(i-1)}$ records the $\seed$ strings in $T_b$ at the nodes on the accepting paths in the previous layer, and so on. To force the properties \ref{enu:tree_invariance_1} and \ref{enu:tree_invariance_2} of the evaluation tree, the hint $CW^{(i)}$ should satisfy the following principles: 
\begin{enumerate}
  \item If a depth-$(i-1)$ parent node is on an accepting path and it has a child node exiting this accepting path, then the corrections for this child node (computed by line \ref{alg:paradigm_correction}) should force the strings at this node in $T_0$ and $T_1$ to be the same. 
  \item For every depth-$(i-1)$ parent node on an accepting path, the $\sign$ corrections for its child that is still on an accepting path should force the $\sign$ strings at this node in $T_0$ and $T_1$ to follow the desired correlation. 
\end{enumerate}
The detailed realization of these principles will be discussed in concrete instantiations. We note that forcing the same strings at each node that exits an accepting suffices for achieving property \ref{enu:tree_invariance_1}: According to the computation of $\Eval_b$, if a parent node is associated with the same strings in $T_0$ and $T_1$, then each of its children is associated with the same strings in $T_0$ and $T_1$, and so is each of the nodes in the subtree rooted at the parent node. 

The paradigm add a convert layer after the last layer of the evaluation tree to convert the strings at the leaf nodes to an element in the output group $\GG$ of $f_{A,B}$. A hint $CW^{(n+1)}$ is associated with the convert layer. The output at a leaf node $x$ with string $\seed\|\sign$ is generated by first computing a pseudorandom $\GG$-element $G_{\sf convert}(\seed)$, then adding to $G_{\sf convert}(\seed)$ a correction computed by ${\sf ConvCorrect}(x, \sign, CW^{(n+1)})$, and then give a sign $(-1)^b$ depending on the party (see line \ref{alg:paradigm_convert_correction}). If the leaf node is not on any accepting path, then $G_{\sf convert}(\seed)$ and the correction should be the same in $T_0$ and $T_1$, which means the outputs in $T_0$ and $T_1$ at this node should add up to $0_\GG$. On the other hand, if the leaf node is on any accepting path, then the hint $CW^{(n+1)}$ given by $\Gen(1^\lambda, \hat{f}_{A,B})$ should yield corrections that force the outputs in $T_0$ and $T_1$ to add up to the corresponding element in $B$. Such $CW^{(n+1)}$ is correctly generated by $\sf GenConvCW$ (see line \ref{alg:paradigm_gen_convert_cw}). 

To sum up, we provide the key generation $\Gen$, single-input evaluation $\Eval$ and full-domain evaluation $\FullEval$ in the paradigm in \cref{fig:DMPF_paradigm}. The computation involves the following methods which will be realized in the next sections: 
\begin{itemize}
  \item $\sf Initialize$ defines the strings at the roots of $T_0,T_1$.
  \item $\sf GenCW$ computes hints $\{CW^{(1)},\cdots CW^{(n)}\}$ associated with $n$ layers that help generate corrections for the strings at the nodes. Two parties use the same set of correction words. 
  \item $\sf GenConvCW$ computes the hint $CW^{(n+1)}$ associated with the convert layer that help genereate corrections for the final output. Two parties use the same set of correction words. 
  \item $\sf Correct$ given a depth-$(i-1)$ parent node, its $\sign$ string and the hint $CW^{(i)}$, outputs an (additive) correction for its children's strings. 
  \item $\sf ConvCorrect$ given a leaf node, its $\sign$ string and the hint $CW^{(n+1)}$, outputs a correction for the final output in the output group $\GG$. 
\end{itemize}

%We provides the big-state DMPF and the OKVS-based DMPF basing on this paradigm, that differ mainly on the way of generating and computing corrections. 

\Yaxin{Mention early termination?}

%We make the following restrictions on the methods in order to guarantee the invariance on the evaluation trees: 
%${\sf M}(\bar{x},\sign, CW) =\sum_{i=1}^l\sign[i]\cdot{\sf M}(\bar{x},0^{i-1}10^{l-i},CW)$ for all $\sf M\in\{Correct, ConvCorrect\}$, input $\bar{x}$ and $CW$.

\begin{figure*}
  \caption{The paradigm of our DMPF schemes. We leave the sign string length $l$, methods $\sf Initialize$, $\sf GenCW$, $\sf GenConvCW$, $\sf Correct$, $\sf ConvCorrect$ to be determined by specific constructions. }
  \label{fig:DMPF_paradigm}
  \fbox{\parbox{\linewidth}{
  \begin{algorithmic}[1]
    \State \textbf{Public parameters: }
    \State The $t$-point function family $\{f_{A,B}\}$ with $t$ an upperbound of the number of nonzero points, input domain $[N]=\{0,1\}^n$ and the output group $\GG$. 
    \State Suppose there is a public PRG $G:\{0,1\}^\lambda\rightarrow \{0,1\}^{2\lambda+2l}$. Parse $G(x) = G_0(x)\|G_1(x)$ to the left half and right half of the output. 
    \State Suppose there is a public PRG $G_{\sf convert}:\{0,1\}^\lambda\rightarrow \GG$. 
    \item[]
    \Procedure{Gen}{$1^\lambda, \hat{f}_{A,B}$}
    \State Denote $A = (\alpha_1,\cdots,\alpha_t)$ in lexicographically order, $B = (\beta_1,\cdots,\beta_t)$. If $|A|<t$, extend $A$ to size-$t$ with arbitrary $\{0,1\}^n$ strings and $B$ with 0's. 
    \State For $0\le i\le n-1$, let $A^{(i)}$ denote the sorted and deduplicated list of $i$-bit prefixes of strings in $A$. Specifically, $A^{(0)} = [\epsilon]$. 
    \State For $0\le i\le n-1$ and $b=0,1$, initialize empty lists $\seed_b^{(i)}$ and $\sign_b^{(i)}$. 
    \State ${\sf Initialize}(\{\seed_b^{(0)},\sign_b^{(0)}\}_{b=0,1})$. 
    \For{$i=1$ to $n$}
    \State $CW^{(i)}\gets {\sf GenCW}(i,A,\{\seed_b^{(i-1)},\sign_b^{(i-1)}\}_{b=0,1})$. \label{alg:paradigm_gen_cw}
      \For{$k = 1$ to $|A^{(i-1)}|$ and $z=0,1$}
        \State Compute $C_{\seed,b}\|C_{\sign^0,b}\|C_{\sign^1,b}\gets {\sf Correct}(A^{(i-1)}[k], \sign_b^{(i-1)}[k], CW^{(i)})$ for $b=0,1$, where $|C_{\seed,b}| = \lambda$ and $|C_{\sign^0,b}| = |C_{\sign^1,b}| = l$. 
        \If{$A^{(i-1)}[k]\|z\in A^{(i)}$}
        \State Append the first $\lambda$ bits of $G_z(\seed_b^{(i-1)}[k])\oplus(C_{\seed,b}\|C_{\sign^z,b})$ to $\seed_b^{(i)}$ and the rest $l$ bits to $\sign_b^{(i)}$. 
        \EndIf
      \EndFor
    \EndFor
    \State $CW^{(n+1)}\gets{\sf GenConvCW}(A,B,\{\seed_b^{(n)},\sign_b^{(n)}\}_{b=0,1})$. \label{alg:paradigm_gen_convert_cw}
    \State Set $k_b \gets (\seed_b^{(0)},\sign_b^{(0)}, CW^{(1)},CW^{(2)},\cdots,CW^{(n+1)})$.
    \State \textbf{return} $(k_0,k_1)$.
    \EndProcedure
    \item[]
    \Procedure{Eval\(_b\)}{$1^\lambda, k_b,x$}
    \State Parse $k_b = ([\seed],[\sign],CW^{(1)},CW^{(2)},\cdots,CW^{(n+1)})$. 
    \State Denote $x=x_1x_2\cdots x_n$. 
    \For{$i = 1$ to $n$}
      \State $C_\seed\|C_{\sign^0}\|C_{\sign^1}\gets {\sf Correct}(x_1\cdots x_{i-1},\sign,CW^{(i)})$, where $|C_\seed| = \lambda$ and $|C_{\sign^0}| = |C_{\sign^1}| = l$. \label{alg:paradigm_correction}
      \State $\seed\|\sign\gets G_{x_i}(\seed)\oplus(C_{\seed}\|C_{\sign^{x_i}})$, where $|\seed|=\lambda$ and $|\sign|=l$. \label{alg:paradigm_seed_sign}
    \EndFor
    \State \Return $(-1)^b\cdot \big(G_{\sf convert}(\seed)+{\sf ConvCorrect}(x,\sign,CW^{(n+1)})\big)$. \label{alg:paradigm_convert_correction}
    \EndProcedure
    \item[]
    \Procedure{FullEval\(_b\)}{$1^\lambda,k_b$}
    \State Parse $k_b=(\seed^{(0)},\sign^{(0)},CW^{(1)},CW^{(2)},\cdots,CW^{(n+1)})$. 
    \State For $1\le i\le n$, ${\sf Path}^{(i)}\gets$ the lexicographical ordered list of $\{0,1\}^i$. ${\sf Path}^{(0)}\gets[\epsilon]$. 
    \State\Yaxin{The evaluation is BFS-style, which costs a lot of memory to store lists $\seed^{(i)}, \sign^{(i)}$. Need a DFS version for large $N$ to reduce memory use? Write in the clear or explain by words?}
    \For{$i=1$ to $n$} 
      \For{$k = 1$ to $2^{i-1}$}
        \State $C_\seed\|C_{\sign^0}\|C_{\sign^1}\gets {\sf Correct}({\sf Path}^{(i-1)}[k],\sign^{(i-1)}[k],CW^{(i)})$, where $|C_\seed| = \lambda$ and $|C_{\sign^0}| = |C_{\sign^1}| = l$. 
        \State $\seed^{(i)}[2k]\|\sign^{(i)}[2k]\gets G_0(\seed^{(i-1)}[k])\oplus (C_\seed\|C_{\sign^0})$, where $|\seed^{(i)}[2k]|=\lambda$ and $|\sign^{(i)}[2k]|=l$. 
        \State $\seed^{(i)}[2k+1]\|\sign^{(i)}[2k+1]\gets G_1(\seed^{(i-1)}[k])\oplus (C_\seed\|C_{\sign^1})$, where $|\seed^{(i)}[2k+1]|=\lambda$ and $|\sign^{(i)}[2k+1]|=l$. 
      \EndFor
    \EndFor
    \For{$k = 1$ to $2^n$}
      \State ${\sf Output}[k]\gets (-1)^b\cdot \big(G_{\sf convert}(\seed^{(n)}[k])+{\sf ConvCorrect}({\sf Path}[k],\sign^{(n)}[k],CW^{(n+1)})\big)$.
    \EndFor
    \State\Return $\sf Output$. 
    \EndProcedure
    \end{algorithmic}}}
\end{figure*}


\newpage
\subsection{Big-State DMPF}\label{sec:big_state_DMPF}
We display our first instantiation of DMPF in \cref{fig:DMPF_big-state}, basing on the paradigm of DMPF in \cref{fig:DMPF_paradigm}. In the big-state DMPF we set the length $l$ of $\sign$ to be $t$, the number of accepting inputs indicated in $\hat{f}_{A,B}$. The evaluation trees $T_0$ and $T_1$ satisfies properties \ref{enu:tree_invariance_1} and \ref{enu:tree_invariance_2}, such that the $\sign$ string at a node stores a share of the unit vector indicating which accepting path this node is on: for a node lying on the $k$th accepting path in the depth-$i$ layer, its $\sign$ strings in $T_0$ and $T_1$ should add up (by bit-wise XOR) to $e_k = 0^{k-1}10^{t-k}$. Then, the (additive) corrections for computing strings at its children generated by line~\ref{alg:big_state_correction} of \cref{fig:DMPF_big-state} equals to $CW^{(i)}[k]$, the $k$th entry of the hint $CW^{(i)}$ associated with this layer. According to line~\ref{alg:big_state_gen_cw} in the construction of $\sf GenCW$, if one of the children exits the accepting path, the $\seed$ correction $C_\seed$ will zero out the difference of this child's $\seed$ strings in $T_0$ and $T_1$. Otherwise $C_\seed$ will be a random correction. The $\sign$ corrections $C_{\sign^0}$ and $C_{\sign^1}$ will force the $\sign$ strings at each child to be a share of $0^t$ if this child exits the accepting path, or to be a unit vector indicating the index of the accepting path in the next layer this child lies on. 

For the convert layer, $\sf GenConvCW$ set $CW^{(n+1)}[k]$ to be the correction that makes the $k$th accepting leaf's outputs in $T_0$ and $T_1$ to add up to $B[k]$. 

We informally argue that the correctness of the big-state DMPF holds since properties \ref{enu:tree_invariance_1} and \ref{enu:tree_invariance_2} of $T_0$ and $T_1$ holds, which in turn gives correct shares of outputs in the end of evaluation. The security holds since (1) the $\seed||\sign$ string at the root of $T_b$ is independent of $A$ and $B$, and (2) each hint $CW^{(i)}$ is masked by the pseudorandom value determined by the other party's key, which is indistinguishable with a truly random hint. 

In the end of this section we briefly discuss about the efficiency of the big-state DMPF, which will be discussed in more details in \cref{sec:applications}. Set the na\"ive solution of DMPF that is a sum of $t$ DPFs as a primary benchmark. The ratio of keysize of the big-state DMPF over the na\"ive solution is roughly $(\lambda+2t)/(\lambda+2)>1$, which is close to 1 if $t\ll \lambda$. $\Gen$, $\Eval$ and $\FullEval$ all traverse one evaluation tree while the na\"ive solution traverse $t$ evaluation trees. However, the PRG used in the big-state DMPF have output length $2\lambda+2t$, which means the running time still grows with $t$. In short, the big-state DMPF is faster than the na\"ive solution with the sacrifice of larger keysize. When $t\ll\lambda$, compared to the na\"ive solution, the big-state DMPF has similar keysize and almost $\times t$ speedup in running time. 

\begin{figure}
  \caption{The parameter $l$ and methods' setting that turns the paradigm of DMPF in~\cref{fig:DMPF_paradigm} into the big-state DMPF. }
  \label{fig:DMPF_big-state}
  \fbox{\parbox{\linewidth}{
  \begin{algorithmic}[1]
    \State Set $l\leftarrow t$, the upperbound of $|A|$. 
    \Procedure{Initialize}{$\{\seed_b^{(0)},\sign_b^{(0)}\}_{b=0,1}$}
    \State For $b=0,1$, let $\seed_b^{(0)} = [r_b]$ where $r_b\xleftarrow{\$}\{0,1\}^\lambda$. 
    \State For $b=0,1$, set $\sign_b^{(0)} = [b\|0^{t-1}]$. 
    \EndProcedure
    \item[]
    \Procedure{GenCW}{$i,A,\{\seed_b^{(i-1)},\sign_b^{(i-1)}\}_{b=0,1}$}
    \State Let $\{A^{(i)}\}_{0\le i\le n}$ be defined as in~\cref{fig:DMPF_paradigm}. 
    \State Sample a list $CW$ of $t$ random strings from $\{0,1\}^{\lambda+2t}$.  
    \For{$k = 1$ to $|A^{(i-1)}|$}
      \State Parse $G(\seed_b^{(i-1)}[k]) = \seed_b^0\|\sign_b^0\|\seed_b^1\|\sign_b^1$, for $b=0,1$, $\seed_b^0,\seed_b^1\in\{0,1\}^\lambda$ and $\sign_b^0,\sign_b^1\in\{0,1\}^t$. 
      \State Compute $\Delta\seed^c = \seed_0^c\oplus\seed_1^c$ and $\Delta \sign^c = \sign_0^c\oplus\sign_1^c$ for $c=0,1$. 
      \State Denote ${\sf path}\leftarrow A^{(i-1)}[k]$. 
      \If{both ${\sf path}\|z$ for $z=0,1$ are in $A^{(i)}$}
        \State $d\gets$ the index of ${\sf path}\|0$ in $A^{(i)}$.
        \State $CW[k]\gets r\|(\Delta\sign^0\oplus e_d) \|(\Delta\sign^1\oplus e_{d+1})$ where $r\xleftarrow{\$}\{0,1\}^\lambda$, $e_d = 0^{d-1}10^{t-d}$. 
      \Else
        \State Let $z$ be such that ${\sf path}\|z\in A^{(i)}$. 
        \State $d\gets$ the index of ${\sf path}\|z$ in $A^{(i)}$. 
        \State $CW[k]\gets 
          \begin{cases}
            \Delta \seed^1\|(\Delta\sign^0\oplus e_d)\|\Delta\sign^1 & z=0\\
            \Delta \seed^0\|\Delta\sign^0\|(\Delta\sign^1\oplus e_d) & z=1
          \end{cases}$.\label{alg:big_state_gen_cw}    
      \EndIf 
    \EndFor
    \State\Return $CW$. 
    \EndProcedure
    \item[]
    \Procedure{GenConvCW}{$A,B,\{\seed_b^{(n)},\sign_b^{(n)}\}$}
      \State Sample a list $CW$ of $t$ random $\GG$-elements.  
      \For{$k = 1$ to $|A|$}
        \State $\Delta g\gets G_{\sf convert}(\seed_0^{(n)}[k]) - G_{\sf convert}(\seed_1^{(n)}[k])$. 
        \State$CW[k]\gets (-1)^{\sign_0^{(n)}[k][k]}(\Delta g-B[k])$.\label{alg:big_state_gen_convert_cw}
      \EndFor
      \State \Return $CW$. 
    \EndProcedure
    \item[]
    \Procedure{Correct}{$\bar{x},\sign,CW$}
      \State \Return $C_{\seed}\|C_{\sign^0}\|C_{\sign^1}\gets\sum_{i=1}^t \sign[i]\cdot CW[i]$, where $C_{\sign^0}$ and $C_{\sign^1}$ are $t$-bit. \label{alg:big_state_correction}
    \EndProcedure
    \item[]
    \Procedure{ConvCorrect}{$x,\sign,CW$}
      \State \Return $\sum_{i=1}^t \sign[i]\cdot CW[i]$. \label{alg:big_state_convert_correction}
    \EndProcedure
  \end{algorithmic}}}
\end{figure}


\subsection{OKVS-based DMPF}\label{sec:OKVS_based_DMPF}
Displayed in \cref{fig:DMPF_OKVS}. 
TBD: explain
\newpage
\begin{figure}
  \caption{The parameter $l$ and methods' setting that turns the paradigm of DMPF in~\cref{fig:DMPF_paradigm} into the OKVS-based DMPF. }
  \label{fig:DMPF_OKVS}
  \fbox{\parbox{\linewidth}{
  \begin{algorithmic}
    \State Set $l\leftarrow 1$. 
    \State For $1\le i\le n$, let $\OKVS_i$ be an OKVS scheme (\cref{def:OKVS}) with key space $\mathcal{K} = \{0,1\}^{i-1}$, value space $\mathcal{V} = \{0,1\}^{\lambda+2}$ and input length $t$. 
    \State let $\OKVS_{\sf convert}$ be an OKVS scheme with key space $\mathcal{K} = \{0,1\}^n$, value space $\mathcal{V} = \GG$ and input length $t$. 
    \item[]
    \Procedure{Initialize}{$\{\seed_b^{(0)},\sign_b^{(0)}\}_{b=0,1}$}
    \State For $b=0,1$, let $\seed_b^{(0)} = [r_b\xleftarrow{\$}\{0,1\}^\lambda]$ and $\sign_b^{(0)} = [b]$. 
    \EndProcedure
    \item[]
    \Procedure{GenCW}{$i,A,\{\seed_b^{(i-1)},\sign_b^{(i-1)}\}_{b=0,1}$}
    \State Let $\{A^{(i)}\}_{0\le i\le n}$ be defined as in~\cref{fig:DMPF_paradigm}. 
    \State Sample a list $V$ of $t$ random strings from $\{0,1\}^{\lambda+2}$.  
    \For{$k = 1$ to $|A^{(i-1)}|$}
      \State Parse $G(\seed_b^{(i-1)}[k]) = \seed_b^0\|\sign_b^0\|\seed_b^1\|\sign_b^1$, for $b=0,1$, $\seed_b^0,\seed_b^1\in\{0,1\}^\lambda$ and $\sign_b^0,\sign_b^1\in\{0,1\}$. 
      \State Compute $\Delta\seed^c = \seed_0^c\oplus\seed_1^c$ and $\Delta \sign^c = \sign_0^c\oplus\sign_1^c$ for $c=0,1$. 
      \State Denote ${\sf path}\leftarrow A^{(i-1)}[k]$. 
      \If{both ${\sf path}\|z$ for $z=0,1$ are in $A^{(i)}$}
        \State $V[k]\gets r\|\Delta\sign^0\oplus 1\|\Delta\sign^1\oplus 1$, where $r\xleftarrow{\$}\{0,1\}^\lambda$. 
      \Else
        \State Let $z$ be such that ${\sf path}\|z\in A^{(i)}$. 
        \State $V[k]\gets \Delta \seed^1\|\Delta\sign^0\oplus (1-z)\|\Delta\sign^1\oplus z$.        
      \EndIf
    \EndFor
    \State \Return $\OKVS_i.\Encode(\{A^{(i-1)}[k], V[k]\}_{1\le k\le |A^{(i-1)}|})$. 
    \EndProcedure
    \item[]
    \Procedure{GenConvCW}{$A,B,\{\seed_b^{(n)},\sign_b^{(n)}\}$}
      \State Sample a list $V$ of $t$ random $\GG$-elements. 
      \For{$k = 1$ to $|A|$}
        \State $\Delta g\gets G_{\sf convert}(\seed_0^{(n)}[k]) - G_{\sf convert}(\seed_1^{(n)}[k])$. 
        \State$V[k]\gets (-1)^{\sign_0^{(n)}[k][k]}(\Delta g-B[k])$.
      \EndFor 
      \State \Return $\OKVS_{\sf convert}(\{A[k], V[k]\}_{1\le k\le t})$. 
    \EndProcedure
    \item[]
    \Procedure{Correct}{$\bar{x}, \sign,CW$}
      \State\Return $C_{\seed}\|C_{\sign^0}\|C_{\sign^1}\gets\sign\cdot\OKVS_i.\Decode(CW, \bar{x})$, where $C_{\sign^0}$ and $C_{\sign^1}$ are bits. 
    \EndProcedure
    \item[]
    \Procedure{ConvCorrect}{$x,\sign,CW$}
      \State \Return $\sign\cdot\OKVS_{\sf convert}.\Decode(CW,x)$. 
    \EndProcedure
  \end{algorithmic}}}
\end{figure}
\Yaxin{One point: the $\row$ matrix of the current layer contains the $\row$ matrix of the previous layers, which might be useful for speedup. }

\subsection{Comparison}
%Comparison table dependent to PRG \& $\FF$-MUL(list the formulas?)\\
%analyze tradeoff\\
%distributed gen advantage
\Cref{tab:formulas_DMPF_comparison} displays the keysize, running time of $\Gen$,$\Eval$ and $\FullEval$ for different DMPF schemes, computed in terms of costs of abstract tools such as PRG, batch code and OKVS. \Yaxin{We can plug in the actual costs of these tools after carrying out a complete experiment. }
	\begin{table*}
    \renewcommand\arraystretch{1.5}
    \scalebox{0.9}{
    \begin{threeparttable}
    \caption{Keysize and running time comparison for different DMPF constructions for domain size $N$, $t$ accepting points, output group $\GG$ and computational security parameter $\lambda$. We leave this table with the abstraction of (probabilistic) batch code in the second column and the abstraction of OKVS in the last column, and plug in concrete instantiations later. $m$ in the second column stands for the number of buckets used in batch code, and $w$ stands for the number of buckets that each input coordinate is mapped to (we only consider regular degree because this is the case in most instantiations). \Yaxin{Denote $T_G$ as the time for computing $G:\{0,1\}^{\lambda+1}\rightarrow \{0,1\}^{2\lambda+2}$, and $T_{G_{\sf convert}}$ as the time for computing $G_{\sf convert}:\{0,1\}^\lambda\rightarrow \GG$. In the last column, denote $\OKVS$ as the $\OKVS$ scheme used for the first $n$ correction words, and $\OKVS_{\sf conv}$ as the $\OKVS$ scheme used for the last correction word. }}
    \label{tab:formulas_DMPF_comparison}
			\begin{tabular}{ccccc}
				\toprule 
        %Header
				 &Sum of $t$ DPFs & Batch code DMPF\cite{cryptoeprint:2019/273,cryptoeprint:2019/1084,cryptoeprint:2021/580,cryptoeprint:2017/1142} & Big-state DMPF & OKVS-based DMPF\\

        \midrule

				Keysize & $t(\lambda+2)\log N+t\log\GG$ & $m(\lambda+2)\log(wN/m)+m\log\GG$ & $t(\lambda+2t)\log N+t\log \GG$ &\makecell{ $\log N\times\OKVS$.Codesize\\$+\OKVS_{\sf conv}$.Codesize}\\

        \cline{1-5}
				
				$Gen()$ & $2t\log N\times T_G+2t\times T_{G_{\sf convert}}$ &\makecell{$2m\log(wN/m)\times T_G+2m\times T_{G_{\sf convert}}$\\Finding a matching of $t$ inputs to $m$ buckets} &$2t\log N\times T_{G^*}$\tnote{1}
        & \makecell{$2t\log N\times T_G+2t\times T_{G_{\sf convert}}$\\$+\log N\times \OKVS.\Encode$\\$+ \OKVS_{\sf conv}.\Encode$} \\

        \cline{1-5}

				$Eval()$ & $t\log N\times T_G+t\times T_{G_{\sf convert}}$ &\makecell{$w\log(wN/m)\times T_G + w\times T_{G_{\sf convert}}$\\Finding all positions the input is mapped to} & $\log N\times T_{G^*} + T_{G_{\sf convert}}$ &\makecell{$\log N\times T_G$ \\$+\log N\times\OKVS.\Decode$\\ $+\OKVS_{\sf conv}.\Decode$} \\

        \cline{1-5}

				$FullEval()$ & $tN\times T_G+tN\times T_{G_{\sf convert}}$ &\makecell{$wN\times T_G+ wN\times T_{G_{\sf convert}}$\\Finding the full domain in buckets} & $N\times T_{G^*} + N\times T_{G_{\sf convert}}$ & \makecell{$N\times T_G$ \\ $+N\times\OKVS.\Decode$\\$+N\times \OKVS_{\sf conv}.\Decode$} \\
        \bottomrule
			\end{tabular}	
      \begin{tablenotes}
        \item [1] The PRG used in big-state DMPF maps from $\{0,1\}^\lambda$ to $\{0,1\}^{2\lambda+2t}$ whose computation time should grow with $t$. We mark this PRG as $G^*$ and its computation time as $T_{G^*}$. 
        \end{tablenotes}
    \end{threeparttable}
    }
	\end{table*}
    \Yaxin{Note that the PRG in big-state DMPF maps from $\{0,1\}^\lambda$ to $\{0,1\}^{2\lambda+2t}$. }

  \Yaxin{Take PCG as a potential application. We care about $\FullEval$ time which is related to PCG seed expanding time. In this aspect, the batch code DMPF consumes $d\times$PRGs than big-state DMPF and OKVS-based DMPF, while big-state DMPF's $\FullEval$ time scales with $t$ and OKVS-based DMPF in addition consumes large field multiplications (in OKVS decoding, and maybe more than this). Therefore we expect different DMPF schemes to be the top choice in different choices of $t$ and depending on the computing time of PRG and large field multiplication. }

  
\subsection{Distributed Key Generation}